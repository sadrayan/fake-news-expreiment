{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "First, we load the sample data IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false          744\n",
      "partly true    655\n",
      "true           157\n",
      "Name: labelCode, dtype: int64\n",
      "(1244, 7)\n",
      "(312, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>polarity</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>labelCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>In May 2017, the ACLU issued a warning for Ame...</td>\n",
       "      <td></td>\n",
       "      <td>2017-05-10</td>\n",
       "      <td>2924</td>\n",
       "      <td>2</td>\n",
       "      <td>[120774, 134939, 143189]</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>Says Tammy Baldwin \"supported legislation allo...</td>\n",
       "      <td>Restoration PAC</td>\n",
       "      <td>2017-03-21</td>\n",
       "      <td>7162</td>\n",
       "      <td>1</td>\n",
       "      <td>[9088, 26681, 11127]</td>\n",
       "      <td>partly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>Eating farm-raised tilapia and other fish from...</td>\n",
       "      <td></td>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>16525</td>\n",
       "      <td>1</td>\n",
       "      <td>[110492, 111520, 117739]</td>\n",
       "      <td>partly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>An MSNBC crew blocked a handicapped parking sp...</td>\n",
       "      <td></td>\n",
       "      <td>2018-11-07</td>\n",
       "      <td>7830</td>\n",
       "      <td>1</td>\n",
       "      <td>[104491, 149735]</td>\n",
       "      <td>partly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>The U.S. has a global trade deficit of $800 bi...</td>\n",
       "      <td></td>\n",
       "      <td>2018-03-09</td>\n",
       "      <td>14266</td>\n",
       "      <td>0</td>\n",
       "      <td>[144325, 144327, 33235, 151147, 151528]</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence         claimant  \\\n",
       "763   In May 2017, the ACLU issued a warning for Ame...                    \n",
       "835   Says Tammy Baldwin \"supported legislation allo...  Restoration PAC   \n",
       "1216  Eating farm-raised tilapia and other fish from...                    \n",
       "559   An MSNBC crew blocked a handicapped parking sp...                    \n",
       "684   The U.S. has a global trade deficit of $800 bi...                    \n",
       "\n",
       "           date     id  polarity                         related_articles  \\\n",
       "763  2017-05-10   2924         2                 [120774, 134939, 143189]   \n",
       "835  2017-03-21   7162         1                     [9088, 26681, 11127]   \n",
       "1216 2018-04-12  16525         1                 [110492, 111520, 117739]   \n",
       "559  2018-11-07   7830         1                         [104491, 149735]   \n",
       "684  2018-03-09  14266         0  [144325, 144327, 33235, 151147, 151528]   \n",
       "\n",
       "        labelCode  \n",
       "763          true  \n",
       "835   partly true  \n",
       "1216  partly true  \n",
       "559   partly true  \n",
       "684         false  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEnCAYAAACnsIi5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAE99JREFUeJzt3X+QndV93/H3x5IxDrYRP7YaIuEINyoeMrEx1ri4cTox1BmDE4s2NvWPBoVRo0xCmHjSGZu0ndpxOw1O01KTNqQkJBF2apsSM6gucUpl0o7Twc7yo2AgHhRsIqn8kG2QCRTbON/+cY/KlSq0d7X36tEevV8zO/c85zl3n+/Omo+Oz57nPqkqJEn9etHQBUiSZsugl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzq0cugCAU089tdatWzd0GZK0rNxxxx1fq6q5hcYdFUG/bt065ufnhy5DkpaVJA9PMs6lG0nqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1Lnjoobpo60dVf8l6FLmKmvXvm2oUuQdBRxRi9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucWDPokZya5e+zrm0nel+TkJLcmebC9ntTGJ8nVSXYkuSfJObP/MSRJL2TBoK+qL1fV2VV1NvB64BngJuAKYHtVrQe2t2OAC4D17WsLcM0sCpckTWaxSzfnA39eVQ8DG4GtrX8rcFFrbwSur5HbgVVJTptKtZKkRVts0L8L+ERrr66qR1r7UWB1a68Bdo69Z1frkyQNYOKgT3Ic8HbgPx14rqoKqMVcOMmWJPNJ5vfs2bOYt0qSFmExM/oLgDur6rF2/Ni+JZn2+njr3w2cPva+ta1vP1V1bVVtqKoNc3Nzi69ckjSRxQT9u3l+2QZgG7CptTcBN4/1X9J235wL7B1b4pEkHWETfUxxkhOAtwA/M9Z9JXBDks3Aw8DFrf8W4EJgB6MdOpdOrVpJ0qJNFPRV9TRwygF9X2e0C+fAsQVcNpXqJElL5p2xktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM5NdMOUdFT50IlDVzA7H9o7dAXqkDN6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ2bKOiTrEpyY5I/S/JAkjcmOTnJrUkebK8ntbFJcnWSHUnuSXLObH8ESdKhTDqj/yjw2ap6NfBa4AHgCmB7Va0HtrdjgAuA9e1rC3DNVCuWJC3KgkGf5ETgbwPXAVTVt6vqSWAjsLUN2wpc1Nobgetr5HZgVZLTpl65JGkik8zozwD2AL+b5K4kv53kBGB1VT3SxjwKrG7tNcDOsffvan2SpAFMEvQrgXOAa6rqdcDTPL9MA0BVFVCLuXCSLUnmk8zv2bNnMW+VJC3CJEG/C9hVVV9oxzcyCv7H9i3JtNfH2/ndwOlj71/b+vZTVddW1Yaq2jA3N3e49UuSFrBg0FfVo8DOJGe2rvOB+4FtwKbWtwm4ubW3AZe03TfnAnvHlngkSUfYpE+Yuhz4/STHAQ8BlzL6R+KGJJuBh4GL29hbgAuBHcAzbawkaSATBX1V3Q1sOMip8w8ytoDLlliXJGlKvDNWkjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6txEQZ/kq0nuTXJ3kvnWd3KSW5M82F5Pav1JcnWSHUnuSXLOLH8ASdKhLWZG/+aqOruq9j0k/Apge1WtB7a3Y4ALgPXtawtwzbSKlSQt3lKWbjYCW1t7K3DRWP/1NXI7sCrJaUu4jiRpCSYN+gL+a5I7kmxpfaur6pHWfhRY3dprgJ1j793V+iRJA1g54bg3VdXuJH8NuDXJn42frKpKUou5cPsHYwvAK1/5ysW8VZK0CBPN6Ktqd3t9HLgJeAPw2L4lmfb6eBu+Gzh97O1rW9+B3/PaqtpQVRvm5uYO/yeQJB3SgkGf5IQkL9/XBn4U+BKwDdjUhm0Cbm7tbcAlbffNucDesSUeSdIRNsnSzWrgpiT7xv/Hqvpskj8FbkiyGXgYuLiNvwW4ENgBPANcOvWqJUkTWzDoq+oh4LUH6f86cP5B+gu4bCrVSZKWzDtjJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM5NHPRJViS5K8ln2vEZSb6QZEeSTyU5rvW/pB3vaOfXzaZ0SdIkFjOj/wXggbHjjwBXVdX3A08Am1v/ZuCJ1n9VGydJGshEQZ9kLfA24LfbcYDzgBvbkK3ARa29sR3Tzp/fxkuSBjDpjP7fAu8H/qodnwI8WVXPteNdwJrWXgPsBGjn97bxkqQBLBj0SX4MeLyq7pjmhZNsSTKfZH7Pnj3T/NaSpDGTzOh/CHh7kq8Cn2S0ZPNRYFWSlW3MWmB3a+8GTgdo508Evn7gN62qa6tqQ1VtmJubW9IPIUl6YQsGfVX9UlWtrap1wLuAz1XVe4HbgHe0YZuAm1t7Wzumnf9cVdVUq5YkTWwp++g/APxikh2M1uCva/3XAae0/l8ErlhaiZKkpVi58JDnVdUfA3/c2g8BbzjImGeBd06hNknSFHhnrCR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktS5BYM+yfFJvpjkfyW5L8kvt/4zknwhyY4kn0pyXOt/STve0c6vm+2PIEk6lElm9N8Czquq1wJnA29Nci7wEeCqqvp+4Algcxu/GXii9V/VxkmSBrJg0NfIX7bDF7evAs4Dbmz9W4GLWntjO6adPz9JplaxJGlRJlqjT7Iiyd3A48CtwJ8DT1bVc23ILmBNa68BdgK083uBU6ZZtCRpchMFfVV9t6rOBtYCbwBevdQLJ9mSZD7J/J49e5b67SRJL2BRu26q6kngNuCNwKokK9uptcDu1t4NnA7Qzp8IfP0g3+vaqtpQVRvm5uYOs3xJ0kIm2XUzl2RVa78UeAvwAKPAf0cbtgm4ubW3tWPa+c9VVU2zaEnS5FYuPITTgK1JVjD6h+GGqvpMkvuBTyb5F8BdwHVt/HXAx5LsAL4BvGsGdUuSJrRg0FfVPcDrDtL/EKP1+gP7nwXeOZXqJElL5p2xktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6N8nDwU9PcluS+5Pcl+QXWv/JSW5N8mB7Pan1J8nVSXYkuSfJObP+ISRJL2ySGf1zwD+qqrOAc4HLkpwFXAFsr6r1wPZ2DHABsL59bQGumXrVkqSJLRj0VfVIVd3Z2k8BDwBrgI3A1jZsK3BRa28Erq+R24FVSU6beuWSpIksao0+yTrgdcAXgNVV9Ug79SiwurXXADvH3rar9UmSBjBx0Cd5GfAHwPuq6pvj56qqgFrMhZNsSTKfZH7Pnj2LeaskaREmCvokL2YU8r9fVZ9u3Y/tW5Jpr4+3/t3A6WNvX9v69lNV11bVhqraMDc3d7j1S5IWMMmumwDXAQ9U1b8ZO7UN2NTam4Cbx/ovabtvzgX2ji3xSJKOsJUTjPkh4CeBe5Pc3fr+MXAlcEOSzcDDwMXt3C3AhcAO4Bng0qlWLElalAWDvqo+D+QFTp9/kPEFXLbEuiRJU+KdsZLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdW6SG6YkaSp+cOsPDl3CTN276d6hSzgoZ/SS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnFgz6JL+T5PEkXxrrOznJrUkebK8ntf4kuTrJjiT3JDlnlsVLkhY2yYz+94C3HtB3BbC9qtYD29sxwAXA+va1BbhmOmVKkg7XgkFfVf8D+MYB3RuBra29FbhorP/6GrkdWJXktGkVK0lavMNdo19dVY+09qPA6tZeA+wcG7er9UmSBrLkP8ZWVQG12Pcl2ZJkPsn8nj17llqGJOkFHG7QP7ZvSaa9Pt76dwOnj41b2/r+P1V1bVVtqKoNc3Nzh1mGJGkhhxv024BNrb0JuHms/5K2++ZcYO/YEo8kaQALPkowySeAHwFOTbIL+CBwJXBDks3Aw8DFbfgtwIXADuAZ4NIZ1CxJWoQFg76q3v0Cp84/yNgCLltqUZKk6fHOWEnqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzMwn6JG9N8uUkO5JcMYtrSJImM/WgT7IC+PfABcBZwLuTnDXt60iSJjOLGf0bgB1V9VBVfRv4JLBxBteRJE1gFkG/Btg5dryr9UmSBrByqAsn2QJsaYd/meTLQ9VyBJwKfO1IXSwfOVJXOiYc0d8dv5wjdqljxJH9b++njvjv7/smGTSLoN8NnD52vLb17aeqrgWuncH1jzpJ5qtqw9B1aPH83S1v/v5GZrF086fA+iRnJDkOeBewbQbXkSRNYOoz+qp6LsnPA38ErAB+p6rum/Z1JEmTmckafVXdAtwyi++9TB0TS1Sd8ne3vPn7A1JVQ9cgSZohPwJBkjpn0EtS5wx66QBJXprkzKHrkKbFoJ+RJG9KcmlrzyU5Y+iatLAkPw7cDXy2HZ+dxO3BWtYM+hlI8kHgA8Avta4XAx8friItwocYfV7TkwBVdTfgP9LLRJK/kWR7ki+149ck+adD1zU0g342/i7wduBpgKr638DLB61Ik/pOVe09oM+tacvHbzGaYH0HoKruYXTT5jHNoJ+Nb9do32oBJDlh4Ho0ufuSvAdYkWR9kl8H/ufQRWli31NVXzyg77lBKjmKGPSzcUOS/wCsSvLTwH9jNNPQ0e9y4AeAbwGfAL4JvG/QirQYX0vy13l+kvUO4JFhSxqeN0zNSJK3AD8KBPijqrp14JKk7iV5FaO7Yf8W8ATwFeAfVNVXh6xraAb9DLSlmmer6rttm96ZwB9W1XcGLk0LSHIbB1mTr6rzBihHh6n9N/iiqnpq6FqOBgb9DCS5A/hh4CTg88A8o3X79w5amBaU5PVjh8cDPwE8V1XvH6gkLUKSf3aw/qr68JGu5Wgy2INHOpeqeibJZuCaqvrVJHcPXZQWVlV3HND1J0kO/OOejl5Pj7WPB34MeGCgWo4aBv1sJMkbgfcCm1vfigHr0YSSnDx2+CLg9cCJA5WjRaqqfz1+nOTXGH1k+jHNoJ+N9zHay3tTVd3X/kB028A1aTJ3MFqjD6NteV/h+X+stfx8D6On3B3TXKOXmiQvAt5YVX8ydC06PEnu5fk/pq8A5oAPV9W/G66q4Rn0U5TkP3OIuyir6u1HsBwdhiR3VdXrhq5DhyfJ+MOynwMeq6pj/oYpl26m69eGLkBLtj3JTwCfLmdBy0qSFYzuWXn10LUcbZzRS2OSPAWcwGg2+CyjtfqqqlcMWpgmkuRm4PKq+ouhazmaOKOfgSTrgV8BzmK0xQuAqnrVYEVpIlXlh88tbycx+ryiLzK21fJYXzY16Gfjd4EPAlcBbwYuxc8VWhaSbK+q8xfq01Fr3975fQJ8ZKBajhoG/Wy8tKq2J0lVPQx8qN0te9C79jS8JMcz2op3apKTGAUEwCuANYMVpsVaWVX/fbwjyUuHKuZoYdDPxrfaVr0Hk/w8sBt42cA16dB+htH9D9/LaC/9vqD/JnBMb81bDpL8LPBzwKuS3DN26uXAMb9d1j/GTlGSj1XVTyZ5P/AbwCrgnzO6s/JXq+r2QQvUgpJcXlW/PnQdWpwkJzJan/8V4IqxU09V1TeGqeroYdBPUZL7gb8D/CHwIzw/KwTA/8FJGoJLN9P1m8B24FU8/3//a+zVXTeSjjhn9DOQ5Jqq+tmh65AkMOil/ST5NHAdowfF/NXQ9UjT4N5uaX+/AbyH0Y6pK9sTwqRlzRm9dBBtF8e7gX8C7GT0cPeP+zhILUfO6KUDJDkF+CngHwJ3AR8FzgF8wLuWJWf00pgkNzF6mPvHgN+rqkfGzs1X1YbBipMOk0EvjUny5qryaWDqikEvAUn+3qHOV9Wnj1Qt0rR5w5Q08uOHOFeAQa9lyxm9NCbJGVX1lYX6pOXEXTfS/v7gIH03HvEqpCly6UYCkrwa+AHgxAPW61/B2FPCpOXIoJdGzmT0ZKJV7L9e/xTw04NUJE2Ja/RSk2QF8IGq+pdD1yJNk2v0UlNV3wUuGroOadqc0UtjklwFvBj4FPD0vv6qunOwoqQlMuilMUkOdldsVdV5R7wYaUoMeknqnLtupAMkeRujrZb/b1tlVX14uIqkpfGPsdKYJL8J/H3gckbP+n0n8H2DFiUtkUs30pgk91TVa8ZeX8bosYI/PHRt0uFyRi/t7/+012eSfC/wHeC0AeuRlsw1eml/n0myCvhXwJ2MPrnyt4YtSVoal26kF5DkJcDxVbV36FqkpTDopTFJjgd+DngTo9n854FrqurZQQuTlsCgl8YkuYHRB5l9vHW9B1hVVe8crippaQx6aUyS+6vqrIX6pOXEXTfS/u5Mcu6+gyR/E5gfsB5pyZzRS2OSPMDos+n/onW9Evgy8Byjz7x5zVC1SYfLoJfGJDnkXbBV9fCRqkWaFoNekjrnGr0kdc6gl6TOGfSS1DmDXpI6Z9BLUuf+L5dJMIIyaABrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_json ('data/train.json')\n",
    "\n",
    "train_df = train_df.sample(frac=.10).reset_index(drop=True)\n",
    "\n",
    "labels = {0:'false', 1:'partly true', 2:'true'}\n",
    "\n",
    "def label(x):\n",
    "    return labels[x]\n",
    "\n",
    "train_df['labelCode'] = train_df.label.apply(label)\n",
    "\n",
    "print(train_df.labelCode.value_counts())\n",
    "train_df.labelCode.value_counts().plot(kind='bar')\n",
    "\n",
    "train_df.rename(columns={\"claim\": \"sentence\", \"label\": \"polarity\"}, inplace=True)\n",
    "\n",
    "train_df.shape\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(train_df, test_size = 0.2, random_state = 0)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A German football club was banned from playing games outside Germany in 1934, after they failed to give a Nazi salute.'] (1244, 1)\n",
      "1\n",
      "(312, 1)\n",
      "(1244, 3) [0. 1. 0.]\n",
      "(312, 3) [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "max_seq_length = 256\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "# Create datasets (Only take up to max_seq_length words for memory)\n",
    "train_text = train_df['sentence'].tolist()\n",
    "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "train_label = train_df['polarity'].tolist()\n",
    "print(train_text[0], train_text.shape)\n",
    "print(train_label[0])\n",
    "\n",
    "test_text = test_df['sentence'].tolist()\n",
    "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "test_label = test_df['polarity'].tolist()\n",
    "print(test_text.shape)\n",
    "\n",
    "\n",
    "encoder.fit(train_label)\n",
    "train_label = encoder.fit_transform(train_label)\n",
    "train_label = np_utils.to_categorical(train_label)\n",
    "print(train_label.shape, train_label[0])\n",
    "test_label = encoder.fit_transform(test_label)\n",
    "test_label = np_utils.to_categorical(test_label)\n",
    "print(test_label.shape, test_label[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "Next, tokenize our text to create `input_ids`, `input_masks`, and `segment_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids   = [0] * max_seq_length\n",
    "        input_mask  = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label       = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "#         np.array(labels).reshape(-1, 1),\n",
    "        np.array(labels),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|██████████| 1244/1244 [00:00<00:00, 2762.76it/s]\n",
      "Converting examples to features: 100%|██████████| 312/312 [00:00<00:00, 2725.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input_ids (1244, 256)\n",
      "train_input_masks (1244, 256)\n",
      "train_segment_ids (1244, 256)\n",
      "train_labels (1244, 3)\n",
      "test_input_ids (312, 256)\n",
      "test_input_masks (312, 256)\n",
      "test_segment_ids (312, 256)\n",
      "test_labels (312, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, train_label)\n",
    "test_examples = convert_text_to_examples(test_text, test_label)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "\n",
    "print('train_input_ids', train_input_ids.shape)\n",
    "print('train_input_masks', train_input_masks.shape)\n",
    "print('train_segment_ids', train_segment_ids.shape)\n",
    "print('train_labels', train_labels.shape)\n",
    "\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)\n",
    "    \n",
    "print('test_input_ids', test_input_ids.shape)\n",
    "print('test_input_masks', test_input_masks.shape)\n",
    "print('test_segment_ids', test_segment_ids.shape)\n",
    "print('test_labels', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(features, labels, shuffle, num_epochs, batch_size):\n",
    "    \"\"\"Generates an input function to be used for model training.\n",
    "    Args:\n",
    "    features: numpy array of features used for training or inference\n",
    "    labels: numpy array of labels for each example\n",
    "    shuffle: boolean for whether to shuffle the data or not (set True for\n",
    "      training, False for evaluation)\n",
    "    num_epochs: number of epochs to provide the data for\n",
    "    batch_size: batch size for training\n",
    "    Returns:\n",
    "    A tf.data.Dataset that can provide data to the Keras model for training or\n",
    "      evaluation\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(features))\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input_ids [  101 26914  2378 11300  2522  1011  6485  1037  2857  2740  2729  3021\n",
      "  2008  2018  2019  3265 11405  1012   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "train_input_masks [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_segment_ids [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_labels [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('train_input_ids', train_input_ids[1])\n",
    "print('train_input_masks', train_input_masks[1])\n",
    "print('train_segment_ids', train_segment_ids[1])\n",
    "print('train_labels', train_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\"Undefined pooling type (must be either first or mean, but is %s\" % self.pooling)\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(self.bert_path, trainable=self.trainable, name=\"%s_module\" % self.name)\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\"Undefined pooling type (must be either first or mean, but is %s\" % self.pooling)\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(\"encoder/layer_{str(11 - %s)}\" % i)\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(\"Undefined pooling type (must be either first or mean, but is %s\" % self.pooling)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
    "    \n",
    "#     encoder\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(3, activation='softmax')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            771         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 110,302,525\n",
      "Trainable params: 788,227\n",
      "Non-trainable params: 109,514,298\n",
      "__________________________________________________________________________________________________\n",
      "1244/1244 [==============================] - 476s 383ms/sample - loss: 0.6303 - acc: 0.6953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efbd62199e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(max_seq_length)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], train_labels,\n",
    "#     validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "    epochs=1, verbose=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_1 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          196864      bert_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            771         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,302,525\n",
      "Trainable params: 788,227\n",
      "Non-trainable params: 109,514,298\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 5min 6s, sys: 4.62 s, total: 5min 11s\n",
      "Wall time: 50.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.save('BertModel.h5')\n",
    "\n",
    "pre_save_preds = model.predict([test_input_ids[0:100], \n",
    "                                test_input_masks[0:100], \n",
    "                                test_segment_ids[0:100]]\n",
    "                              ) # predictions before we clear and reload model\n",
    "\n",
    "# Clear and load model\n",
    "model = None\n",
    "model = build_model(max_seq_length)\n",
    "initialize_vars(sess)\n",
    "model.load_weights('BertModel.h5')\n",
    "\n",
    "# post_save_preds = model.predict([test_input_ids[0:100], \n",
    "#                                 test_input_masks[0:100], \n",
    "#                                 test_segment_ids[0:100]]\n",
    "#                               ) # predictions after we clear and reload model\n",
    "# all(pre_save_preds == post_save_preds) # Are they the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate([test_input_ids, \n",
    "                                test_input_masks, \n",
    "                                test_segment_ids], test_labels,\n",
    "                               verbose=2)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('Loss: %f' % loss)\n",
    "\n",
    "print(post_save_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 28s, sys: 16.2 s, total: 15min 44s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "y_preds = model.predict([test_input_ids, \n",
    "                                test_input_masks, \n",
    "                                test_segment_ids])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3660267  0.5337784  0.10019479] (312, 3)\n",
      "[1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0\n",
      " 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1\n",
      " 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1\n",
      " 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
      " 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0\n",
      " 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 0\n",
      " 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
      " 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0\n",
      " 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1] (312,)\n",
      "[0. 0. 1.] (312, 3)\n",
      "[[2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [1]]\n",
      "sklearn Macro-F1-Score: 0.41192569505822524\n",
      "sklearn Micro-F1-Score: 0.5897435897435898\n",
      "sklearn weighted-F1-Score: 0.5524107865975336\n",
      "sklearn no average-F1-Score: [0.57915058 0.65662651 0.        ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.54      0.58       140\n",
      "           1       0.56      0.78      0.66       139\n",
      "           2       0.00      0.00      0.00        33\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       312\n",
      "   macro avg       0.40      0.44      0.41       312\n",
      "weighted avg       0.53      0.59      0.55       312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(y_preds[0], y_preds.shape)\n",
    "y_preds = np.argmax(y_preds, axis=1)\n",
    "print(y_preds, y_preds.shape)\n",
    "print(test_labels[0], test_labels.shape)\n",
    "# Results\n",
    "y_true = np.argmax(test_labels, axis=1)\n",
    "y_preds = y_preds.reshape((y_preds.shape[0], 1))\n",
    "y_true = y_true.reshape((y_true.shape[0], 1))\n",
    "print(y_true)\n",
    "# print(y_true.reshape((y_true.shape[0], 1)).shape)\n",
    "print('sklearn Macro-F1-Score:', f1_score(y_true, y_preds, average='macro'))\n",
    "print('sklearn Micro-F1-Score:', f1_score(y_true, y_preds, average='micro'))  \n",
    "print('sklearn weighted-F1-Score:', f1_score(y_true, y_preds, average='weighted'))  \n",
    "print('sklearn no average-F1-Score:', f1_score(y_true, y_preds, average=None))\n",
    "\n",
    "print(classification_report(y_true, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to:  b'keras_export/1573943894'\n"
     ]
    }
   ],
   "source": [
    "# Export the model to a local SavedModel directory \n",
    "export_path = tf.contrib.saved_model.save_keras_model(model, 'keras_export')\n",
    "print(\"Model exported to: \", export_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [ml_engine/local_python].\n"
     ]
    }
   ],
   "source": [
    "# GCloud\n",
    "! export GOOGLE_APPLICATION_CREDENTIALS=\"/home/sonic/leadersprize/fakenews-40cea3fac9e2.json\"\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/sonic/leadersprize/fakenews-40cea3fac9e2.json'\n",
    "# %env GOOGLE_APPLICATION_CREDENTIALS '/home/sonic/leadersprize/fakenews-40cea3fac9e2.json'\n",
    "\n",
    "PROJECT_ID = \"fakenews-259222\" #@param {type:\"string\"}\n",
    "BUCKET_NAME =  PROJECT_ID + \"-model\" #@param {type:\"string\"}\n",
    "REGION = \"us-central1\" #@param {type:\"string\"\n",
    "\n",
    "JOB_NAME = 'my_first_keras_job'\n",
    "JOB_DIR = 'gs://' + BUCKET_NAME + '/keras-job-dir'\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "# Explicitly tell `gcloud ai-platform local train` to use Python 3 \n",
    "! gcloud config set ml_engine/local_python $(which python3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sonic/leadersprize\n",
      "%GOOGLE_APPLICATION_CREDENTIALS%\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! echo %GOOGLE_APPLICATION_CREDENTIALS%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://fakenews-259222-model/...\n",
      "ServiceException: 409 Bucket fakenews-259222-model already exists.\n",
      "                                 gs://fakenews-259222-model/keras-job-dir/\n"
     ]
    }
   ],
   "source": [
    "# Only if your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket.\n",
    "! gsutil mb -l $REGION gs://$BUCKET_NAME\n",
    "# ! gsutil acl set -R project-private gs://$BUCKET_NAME\n",
    "! gsutil ls -al gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to:  b'gs://fakenews-259222-model/keras-job-dir/keras_export/1573947349'\n"
     ]
    }
   ],
   "source": [
    "# Export the model to a SavedModel directory in Cloud Storage\n",
    "\n",
    "export_path = tf.contrib.saved_model.save_keras_model(model, JOB_DIR + '/keras_export')\n",
    "print(\"Model exported to: \", export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ml engine model [projects/fakenews-259222/models/my_first_keras_model].\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"my_first_keras_model\"\n",
    "\n",
    "! gcloud ai-platform models create $MODEL_NAME \\\n",
    "  --regions $REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: option -l not recognized\r\n"
     ]
    }
   ],
   "source": [
    "! gsutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/train.json [Content-Type=application/json]...\n",
      "- [1/1 files][  4.4 MiB/  4.4 MiB] 100% Done                                    \n",
      "Operation completed over 1 objects/4.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Update datasets\n",
    "! gsutil -m cp -r data/train.json gs://$BUCKET_NAME/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://fakenews-259222-model/data/train.json\n",
      "gs://fakenews-259222-model/data/train.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td></td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td></td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td></td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td></td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim         claimant  \\\n",
       "0  A line from George Orwell's novel 1984 predict...                    \n",
       "1  Maine legislature candidate Leslie Gibson insu...                    \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...                    \n",
       "3  In 1988 author Roald Dahl penned an open lette...                    \n",
       "4  When it comes to fighting terrorism, \"Another ...  Hillary Clinton   \n",
       "\n",
       "        date  id  label                            related_articles  \n",
       "0 2017-07-17   0      0            [122094, 122580, 130685, 134765]  \n",
       "1 2018-03-17   1      2                    [106868, 127320, 128060]  \n",
       "2 2018-07-18   4      1                    [132130, 132132, 149722]  \n",
       "3 2019-02-04   5      2                    [123254, 123418, 127464]  \n",
       "4 2016-03-22   6      2  [41099, 89899, 72543, 82644, 95344, 88361]  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storage directory\n",
    "import tempfile\n",
    "DATA_DIR = os.path.join(tempfile.gettempdir(), 'fakenews_data')\n",
    "\n",
    "# Download options.\n",
    "# https://storage.cloud.google.com/[BUCKET_NAME]/[OBJECT_NAME]\n",
    "DATA_URL = ('gs://%s/data' % BUCKET_NAME)\n",
    "TRAINING_FILE = 'train.json'\n",
    "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
    "\n",
    "print(TRAINING_URL)\n",
    "\n",
    "if (tf.io.gfile.exists(TRAINING_URL)):\n",
    "    print(TRAINING_URL)\n",
    "    \n",
    "tf.io.gfile.copy(\n",
    "    TRAINING_URL,\n",
    "    'datadd/',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "with tf.io.gfile.GFile(TRAINING_URL, \"rb\") as file:\n",
    "#     options_dict = (file.read().decode(\"utf-8\"))\n",
    "    train_df = pd.read_json (file.read())\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.create) ALREADY_EXISTS: Field: version.name Error: A version with the same name already exists.\r\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\r\n",
      "  fieldViolations:\r\n",
      "  - description: A version with the same name already exists.\r\n",
      "    field: version.name\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_VERSION = \"v1\"\n",
    "\n",
    "# Get a list of directories in the `keras_export` parent directory\n",
    "KERAS_EXPORT_DIRS = ! gsutil ls $JOB_DIR/keras_export/\n",
    "\n",
    "# Pick the directory with the latest tiAmestamp, in case you've trained\n",
    "# multiple times\n",
    "SAVED_MODEL_PATH = KERAS_EXPORT_DIRS[-1]\n",
    "\n",
    "# Create model version based on that SavedModel directory\n",
    "! gcloud ai-platform versions create $MODEL_VERSION \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --python-version 3.5 \\\n",
    "  --framework tensorflow \\\n",
    "  --origin $SAVED_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"instances\": [[[101, 2019, 5334, 6458, 7219, 5895, 1005, 7579, 21046, 3070, 2055, 1996, 3684, 2011, 7302, 2041, 2008, 3548, 3529, 1999, 5712, 11997, 2007, 2714, 3785, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]}"
     ]
    }
   ],
   "source": [
    "# test_input_ids[:1]\n",
    "import json\n",
    "\n",
    "! rm prediction_input.json\n",
    "\n",
    "prediction_json = { \"instances\": [test_input_ids[:1].tolist(),\n",
    "                                            test_input_masks[:1].tolist(), \n",
    "                                            test_segment_ids[:1].tolist()] }\n",
    "# prediction_json = []\n",
    "# prediction_df = pd.DataFrame(prediction_json)\n",
    "# prediction_df.to_json('prediction_input.json')\n",
    "\n",
    "# with open('prediction_input.json', 'w') as json_file:\n",
    "#     for row in prediction_df.values.tolist():\n",
    "#         json.dump(row, json_file)\n",
    "#         json_file.write('\\n')\n",
    "\n",
    "# with open(\"prediction_input.json\", \"w\") as write_file:\n",
    "#     json.dump(prediction_json, write_file)\n",
    "\n",
    "\n",
    "predict_instance_json = \"prediction_input.json\"\n",
    "predict_instance_tfr = \"inputs.tfr\"\n",
    "\n",
    "with open(predict_instance_json, \"wb\") as fp:\n",
    "        fp.write(json.dumps(prediction_json).encode())\n",
    "    \n",
    "    \n",
    "! cat prediction_input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  DEFAULT_VERSION_NAME\r\n",
      "my_first_keras_model  v1\r\n"
     ]
    }
   ],
   "source": [
    "{\"instances\": prediction_json}\n",
    "! gcloud ai-platform models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36602676, 0.5337784 , 0.1001948 ],\n",
       "       [0.5724356 , 0.38378447, 0.04377986],\n",
       "       [0.42317846, 0.5072334 , 0.06958818],\n",
       "       [0.1621002 , 0.77673244, 0.06116732],\n",
       "       [0.30797893, 0.60937566, 0.08264544],\n",
       "       [0.30571237, 0.6302835 , 0.06400414],\n",
       "       [0.17362562, 0.76148766, 0.06488673],\n",
       "       [0.2787502 , 0.61786336, 0.10338645],\n",
       "       [0.6784693 , 0.26468378, 0.05684688],\n",
       "       [0.46706417, 0.4457351 , 0.08720069]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model.predict(prediction_json)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"error\": \"Prediction failed: Unexpected tensor name: instances;;;;;;;;\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform predict \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --version $MODEL_VERSION \\\n",
    "  --json-instances prediction_input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is similar to `python -m trainer.task --job-dir local-training-output`\n",
    "# but it better replicates the AI Platform environment, especially for\n",
    "# distributed training (not applicable here).\n",
    "! gcloud ai-platform local train \\\n",
    "  --package-path trainer \\\n",
    "  --module-name trainer.task \\\n",
    "  --job-dir local-training-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete model version resource\n",
    "! gcloud ai-platform versions delete $MODEL_VERSION --quiet --model $MODEL_NAME \n",
    "\n",
    "# Delete model resource\n",
    "! gcloud ai-platform models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $JOB_DIR\n",
    "\n",
    "# If the training job is still running, cancel it\n",
    "! gcloud ai-platform jobs cancel $JOB_NAME --quiet --verbosity critical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
