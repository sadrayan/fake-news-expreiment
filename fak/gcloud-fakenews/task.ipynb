{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --job-dir JOB_DIR [--num-epochs NUM_EPOCHS]\n",
      "                             [--batch-size BATCH_SIZE]\n",
      "                             [--learning-rate LEARNING_RATE]\n",
      "                             [--verbosity {DEBUG,ERROR,FATAL,INFO,WARN}]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --job-dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Trains a Keras model to predict income bracket from other Census data.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from trainer import model\n",
    "from trainer import util\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "    \n",
    "PROJECT_ID = \"fakenews-259222\" #@param {type:\"string\"}\n",
    "BUCKET_NAME =  PROJECT_ID + \"-model\" #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "    Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "      '--job-dir',\n",
    "      type=str,\n",
    "      required=True,\n",
    "      help='local or GCS location for writing checkpoints and exporting models')\n",
    "    parser.add_argument(\n",
    "      '--bucket-name',\n",
    "      type=str,\n",
    "      required=True,\n",
    "      help='GCS BUCKETNAME')\n",
    "    parser.add_argument(\n",
    "      '--num-epochs',\n",
    "      type=int,\n",
    "      default=20,\n",
    "      help='number of times to go through the data, default=20')\n",
    "    parser.add_argument(\n",
    "      '--batch-size',\n",
    "      default=128,\n",
    "      type=int,\n",
    "      help='number of records to read during each training step, default=128')\n",
    "    parser.add_argument(\n",
    "      '--learning-rate',\n",
    "      default=.01,\n",
    "      type=float,\n",
    "      help='learning rate for gradient descent, default=.01')\n",
    "    parser.add_argument(\n",
    "      '--verbosity',\n",
    "      choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "      default='INFO')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "#model training\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "    \n",
    "x\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "\n",
    "    Args:\n",
    "    args: dictionary of arguments - see get_args() for details\n",
    "    \"\"\"\n",
    "\n",
    "    args.bucket_name\n",
    "    train_x, train_y, eval_x, eval_y = util.load_data(BUCKET_NAME)\n",
    "\n",
    "    # dimensions\n",
    "    num_train_examples, input_dim = train_x.shape\n",
    "    num_eval_examples = eval_x.shape[0]\n",
    "\n",
    "    # Create the Keras Model\n",
    "    keras_model = model.create_keras_model(nb_words, embedding_matrix,  embed_dim = 300)\n",
    "\n",
    "    # Pass a numpy array by passing DataFrame.values\n",
    "    training_dataset = model.input_fn(\n",
    "          features=train_x.values,\n",
    "          labels=train_y,\n",
    "          shuffle=True,\n",
    "          num_epochs=args.num_epochs,\n",
    "          batch_size=args.batch_size)\n",
    "\n",
    "    # Pass a numpy array by passing DataFrame.values\n",
    "    validation_dataset = model.input_fn(\n",
    "          features=eval_x.values,\n",
    "          labels=eval_y,\n",
    "          shuffle=False,\n",
    "          num_epochs=args.num_epochs,\n",
    "          batch_size=num_eval_examples)\n",
    "\n",
    "    # Setup Learning Rate decay.\n",
    "#     lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
    "#       lambda epoch: args.learning_rate + 0.02 * (0.5 ** (1 + epoch)),\n",
    "#       verbose=True)\n",
    "\n",
    "    # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "        os.path.join(args.job_dir, 'keras_tensorboard'), histogram_freq=1)\n",
    "\n",
    "    # Train model\n",
    "#     keras_model.fit(\n",
    "#       training_dataset,\n",
    "#       steps_per_epoch=int(num_train_examples / args.batch_size),\n",
    "#       epochs=args.num_epochs,\n",
    "#       validation_data=validation_dataset,\n",
    "#       validation_steps=1,\n",
    "#       verbose=1,\n",
    "#       callbacks=[lr_decay_cb, tensorboard_cb])\n",
    "    \n",
    "    # Instantiate variables\n",
    "    initialize_vars(sess)\n",
    "\n",
    "\n",
    "    hist = model.fit({'claim': train_claim_seq, 'article': train_article_seq},\n",
    "              y_train, callbacks=[tensorboard_cb],\n",
    "              epochs=args.num_epochs,\n",
    "              validation_split=0.1, \n",
    "              verbose=1,\n",
    "              batch_size=args.batch_size)\n",
    "\n",
    "    export_path = os.path.join(args.job_dir, 'keras_export')\n",
    "    tf.contrib.saved_model.save_keras_model(keras_model, export_path)\n",
    "    print('Model exported to: {}'.format(export_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    tf.logging.set_verbosity(args.verbosity)\n",
    "    train_and_evaluate(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
