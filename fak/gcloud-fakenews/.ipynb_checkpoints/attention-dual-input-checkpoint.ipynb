{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = {0:'false', 1:'partly true', 2:'true'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>article</th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19415</th>\n",
       "      <td>50816</td>\n",
       "      <td>medicaid gov medicaid marketplace overview fed...</td>\n",
       "      <td>single mother two qualify basic healthcare med...</td>\n",
       "      <td>Chris Koster</td>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19416</th>\n",
       "      <td>58918</td>\n",
       "      <td>##th amendment constitution direct election se...</td>\n",
       "      <td>fake news media wants speak house midterm resu...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19417</th>\n",
       "      <td>23492</td>\n",
       "      <td>robert mueller former director named special c...</td>\n",
       "      <td>fbi announced inquiry russian ties trump campa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19418</th>\n",
       "      <td>31249</td>\n",
       "      <td>points marco rubio troubled financial history ...</td>\n",
       "      <td>says question financial skills cnbc debate inc...</td>\n",
       "      <td>Marco Rubio</td>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19419</th>\n",
       "      <td>3703</td>\n",
       "      <td>transcripts return transcripts main page state...</td>\n",
       "      <td>president george bush spent trillion tax cuts ...</td>\n",
       "      <td>Donna Brazile</td>\n",
       "      <td>2009-08-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            article  \\\n",
       "19415       50816  medicaid gov medicaid marketplace overview fed...   \n",
       "19416       58918  ##th amendment constitution direct election se...   \n",
       "19417       23492  robert mueller former director named special c...   \n",
       "19418       31249  points marco rubio troubled financial history ...   \n",
       "19419        3703  transcripts return transcripts main page state...   \n",
       "\n",
       "                                                   claim       claimant  \\\n",
       "19415  single mother two qualify basic healthcare med...   Chris Koster   \n",
       "19416  fake news media wants speak house midterm resu...   Donald Trump   \n",
       "19417  fbi announced inquiry russian ties trump campa...            NaN   \n",
       "19418  says question financial skills cnbc debate inc...    Marco Rubio   \n",
       "19419  president george bush spent trillion tax cuts ...  Donna Brazile   \n",
       "\n",
       "             date  label  \n",
       "19415  2016-01-20      1  \n",
       "19416  2018-11-19      0  \n",
       "19417  2017-10-27      2  \n",
       "19418  2015-10-28      0  \n",
       "19419  2009-08-02      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_csv('../data/full_train.cvs')\n",
    "train_df = train_df.sample(frac=.25).reset_index(drop=True)\n",
    "\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train:  (14565, 2)\n",
      "num test:  (14565, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>prohibition ended police deaths reach ### year...</td>\n",
       "      <td>scope marijuana use united states marijuana co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>photograph shows hercules world biggest dog se...</td>\n",
       "      <td>## stone dog world heaviest proud owner tom sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>oil companies obtain acre public land less pri...</td>\n",
       "      <td>fair share companies sit public lands less cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>casey anthony opening home daycare center flor...</td>\n",
       "      <td>false motorcycle curfew ## states march #### s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>governing magazine estimated states expand med...</td>\n",
       "      <td>rural hospitals life support story part specia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   claim  \\\n",
       "11284  prohibition ended police deaths reach ### year...   \n",
       "11964  photograph shows hercules world biggest dog se...   \n",
       "5390   oil companies obtain acre public land less pri...   \n",
       "860    casey anthony opening home daycare center flor...   \n",
       "15795  governing magazine estimated states expand med...   \n",
       "\n",
       "                                                 article  \n",
       "11284  scope marijuana use united states marijuana co...  \n",
       "11964  ## stone dog world heaviest proud owner tom sc...  \n",
       "5390   fair share companies sit public lands less cup...  \n",
       "860    false motorcycle curfew ## states march #### s...  \n",
       "15795  rural hospitals life support story part specia...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "X = train_df[['claim', 'article']]\n",
    "y = train_df['label']\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"num train: \", X_train.shape)\n",
    "print(\"num test: \", y_train.shape)\n",
    "\n",
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  133883\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as sw\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "max_seq_len = 300\n",
    "label_names = [\"true\", \"almost\", \"false\"]\n",
    "\n",
    "num_classes = len(label_names)\n",
    "\n",
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train['claim'].tolist() + X_train['article'].tolist() )\n",
    "\n",
    "train_claim_seq   = tokenizer.texts_to_sequences(X_train['claim'].tolist())\n",
    "train_article_seq = tokenizer.texts_to_sequences(X_train['article'].tolist())\n",
    "\n",
    "test_claim_seq   = tokenizer.texts_to_sequences(X_test['claim'].tolist())\n",
    "text_article_seq = tokenizer.texts_to_sequences(X_test['article'].tolist())\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)) + 1\n",
    "\n",
    "#pad sequences\n",
    "train_claim_seq   = pad_sequences(train_claim_seq, maxlen=max_seq_len)\n",
    "train_article_seq = pad_sequences(train_article_seq, maxlen=max_seq_len)\n",
    "\n",
    "test_claim_seq    = pad_sequences(test_claim_seq, maxlen=max_seq_len)\n",
    "test_article_seq  = pad_sequences(text_article_seq, maxlen=max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 4 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1521it [00:00, 15199.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [01:11, 14043.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999995 word vectors\n",
      "preparing embedding matrix...\n",
      "number of null word embeddings: 36863\n",
      "sample words not found:  ['mugonyi' 'spendathon' 'hininger' 'gyor' 'nishio' 'oersteds' 'bliley'\n",
      " 'cutor' 'lepage' 'gualtieri']\n"
     ]
    }
   ],
   "source": [
    "#load embeddings\n",
    "import os, re, csv, math, codecs\n",
    "\n",
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/home/sonic/.keras/datasets/wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "claim (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "article (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "claim_emb (Embedding)           (None, None, 300)    30000300    claim[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "article_emb (Embedding)         (None, None, 300)    30000300    article[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 128)          219648      claim_emb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          219648      article_emb[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat-layer (Concatenate)      (None, 256)          0           lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "priority (Dense)                (None, 3)            771         concat-layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 60,440,667\n",
      "Trainable params: 440,067\n",
      "Non-trainable params: 60,000,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# num_tags = 12  # Number of unique issue tags\n",
    "# num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
    "# num_departments = 4  # Number of departments for predictions\n",
    "\n",
    "claim_input   = keras.Input(shape=(None,), name='claim')    # Variable-length sequence of ints\n",
    "article_input = keras.Input(shape=(None,), name='article')  # Variable-length sequence of ints\n",
    "\n",
    "# Embed each word into vector\n",
    "claim_features   = layers.Embedding(nb_words, embed_dim, weights=[embedding_matrix], \n",
    "                                  input_length=max_seq_len, trainable=False, name='claim_emb')(claim_input)\n",
    "# Embed each word into vector\n",
    "article_features = layers.Embedding(nb_words, embed_dim, weights=[embedding_matrix], \n",
    "                                 input_length=max_seq_len, trainable=False, name='article_emb')(article_input)\n",
    "\n",
    "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "claim_features = layers.LSTM(128)(claim_features)\n",
    "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "article_features = layers.LSTM(128)(article_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = layers.concatenate([claim_features, article_features], name='concat-layer')\n",
    "\n",
    "# Stick a logistic regression for priority prediction on top of the features\n",
    "priority_pred = layers.Dense(num_classes, activation='softmax', name='priority')(x)\n",
    "# Stick a department classifier on top of the features\n",
    "# department_pred = layers.Dense(num_departments, activation='softmax', name='department')(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting both priority and department\n",
    "model = keras.Model(inputs=[claim_input, article_input], outputs=[priority_pred], name='fake-model')\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "keras.utils.plot_model(model, 'multi_input_model.png', show_shapes=True)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13108 samples, validate on 1457 samples\n",
      "Epoch 1/4\n",
      "  768/13108 [>.............................] - ETA: 1:40 - loss: 1.0601 - categorical_accuracy: 0.4596"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "    \n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "\n",
    "hist = model.fit({'claim': train_claim_seq, 'article': train_article_seq},\n",
    "          y_train,\n",
    "          epochs=num_epochs, validation_split=0.1,verbose=1,\n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist):\n",
    "    #generate plots\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
    "    plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Cross-Entropy Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\n",
    "#     plt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend(loc='upper left')\n",
    "#     plt.show()\n",
    "\n",
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict({'claim': test_claim_seq, 'article': test_article_seq})\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Results\n",
    "print('sklearn Macro-F1-Score:',      f1_score(y_test, y_pred, average='macro'))\n",
    "print('sklearn Micro-F1-Score:',      f1_score(y_test, y_pred, average='micro'))  \n",
    "print('sklearn weighted-F1-Score:',   f1_score(y_test, y_pred, average='weighted'))  \n",
    "print('sklearn no average-F1-Score:', f1_score(y_test, y_pred, average=None))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to a local SavedModel directory \n",
    "export_path = tf.contrib.saved_model.save_keras_model(model, 'keras_export')\n",
    "print(\"Model exported to: \", export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCloud\n",
    "! export GOOGLE_APPLICATION_CREDENTIALS=\"/home/sonic/leadersprize/fakenews-40cea3fac9e2.json\"\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/sonic/leadersprize/fakenews-40cea3fac9e2.json'\n",
    "# %env GOOGLE_APPLICATION_CREDENTIALS '/home/sonic/leadersprize/fakenews-40cea3fac9e2.json'\n",
    "\n",
    "PROJECT_ID = \"fakenews-259222\" #@param {type:\"string\"}\n",
    "BUCKET_NAME =  PROJECT_ID + \"-model\" #@param {type:\"string\"}\n",
    "REGION = \"us-central1\" #@param {type:\"string\"\n",
    "\n",
    "JOB_NAME = 'my_first_keras_job'\n",
    "JOB_DIR = 'gs://' + BUCKET_NAME + '/keras-job-dir'\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "# Explicitly tell `gcloud ai-platform local train` to use Python 3 \n",
    "! gcloud config set ml_engine/local_python $(which python3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket.\n",
    "! gsutil mb -l $REGION gs://$BUCKET_NAME\n",
    "# ! gsutil acl set -R project-private gs://$BUCKET_NAME\n",
    "! gsutil ls -al gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to a SavedModel directory in Cloud Storage\n",
    "export_path = tf.contrib.saved_model.save_keras_model(model, JOB_DIR + '/keras_export')\n",
    "print(\"Model exported to: \", export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"my_first_keras_model\"\n",
    "MODEL_VERSION = \"v1\"\n",
    "\n",
    "! gcloud ai-platform models create $MODEL_NAME \\\n",
    "  --regions $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of directories in the `keras_export` parent directory\n",
    "KERAS_EXPORT_DIRS = ! gsutil ls\n",
    "print(KERAS_EXPORT_DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of directories in the `keras_export` parent directory\n",
    "KERAS_EXPORT_DIRS = export_path.decode(\"utf-8\") \n",
    "print (KERAS_EXPORT_DIRS)\n",
    "\n",
    "# Pick the directory with the latest tiAmestamp, in case you've trained\n",
    "# multiple times\n",
    "SAVED_MODEL_PATH = KERAS_EXPORT_DIRS\n",
    "\n",
    "print('saved model path', SAVED_MODEL_PATH)\n",
    "\n",
    "# Create model version based on that SavedModel directory\n",
    "! gcloud ai-platform versions create $MODEL_VERSION \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --python-version 3.5 \\\n",
    "  --framework tensorflow \\\n",
    "  --origin $SAVED_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete model version resource\n",
    "! gcloud ai-platform versions delete $MODEL_VERSION --quiet --model $MODEL_NAME \n",
    "\n",
    "# Delete model resource\n",
    "! gcloud ai-platform models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $JOB_DIR\n",
    "\n",
    "# If the training job is still running, cancel it\n",
    "! gcloud ai-platform jobs cancel $JOB_NAME --quiet --verbosity critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
