{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defines a Keras model and input function for training.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#load embeddings\n",
    "import os, re, csv, math, codecs\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 4 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def input_fn(features, labels, shuffle, num_epochs, batch_size):\n",
    "    \"\"\"Generates an input function to be used for model training.\n",
    "    Args:\n",
    "    features: numpy array of features used for training or inference\n",
    "    labels: numpy array of labels for each example\n",
    "    shuffle: boolean for whether to shuffle the data or not (set True for\n",
    "      training, False for evaluation)\n",
    "    num_epochs: number of epochs to provide the data for\n",
    "    batch_size: batch size for training\n",
    "    Returns:\n",
    "    A tf.data.Dataset that can provide data to the Keras model for training or\n",
    "      evaluation\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(features))\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding():\n",
    "    print('loading word embeddings...')\n",
    "    embeddings_index = {}\n",
    "    f = codecs.open('/home/sonic/.keras/datasets/wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "    #embedding matrix\n",
    "    print('preparing embedding matrix...')\n",
    "    words_not_found = []\n",
    "\n",
    "    embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_keras_model(nb_words, embedding_matrix,  embed_dim = 300):\n",
    "    \"\"\"Creates Keras Model for  Classification.\n",
    "    Args:\n",
    "    nb_words: How many features the input has\n",
    "    embed_dim: embedding dimension = 300\n",
    "    embedding_matrix: embedding weights\n",
    "    Returns:\n",
    "    The compiled Keras model (still needs to be trained)\n",
    "    \"\"\"\n",
    "\n",
    "    claim_input   = keras.Input(shape=(None,), name='claim')    # Variable-length sequence of ints\n",
    "    article_input = keras.Input(shape=(None,), name='article')  # Variable-length sequence of ints\n",
    "\n",
    "    # Embed each word into vector\n",
    "    claim_features   = layers.Embedding(nb_words, embed_dim, weights=[embedding_matrix], \n",
    "                                      input_length=max_seq_len, trainable=False, name='claim_emb')(claim_input)\n",
    "    # Embed each word into vector\n",
    "    article_features = layers.Embedding(nb_words, embed_dim, weights=[embedding_matrix], \n",
    "                                     input_length=max_seq_len, trainable=False, name='article_emb')(article_input)\n",
    "\n",
    "    # Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "    claim_features = layers.LSTM(128)(claim_features)\n",
    "    # Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "    article_features = layers.LSTM(128)(article_features)\n",
    "\n",
    "    # Merge all available features into a single large vector via concatenation\n",
    "    x = layers.concatenate([claim_features, article_features], name='concat-layer')\n",
    "\n",
    "    # Stick a logistic regression for priority prediction on top of the features\n",
    "    priority_pred = layers.Dense(num_classes, activation='softmax', name='priority')(x)\n",
    "    # Stick a department classifier on top of the features\n",
    "    # department_pred = layers.Dense(num_departments, activation='softmax', name='department')(x)\n",
    "\n",
    "    # Instantiate an end-to-end model predicting both priority and department\n",
    "    model = keras.Model(inputs=[claim_input, article_input], outputs=[priority_pred], name='fake-model')\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "                  loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    keras.utils.plot_model(model, 'multi_input_model.png', show_shapes=True)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
