{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utilities to download and preprocess the FakeNews data.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ipython nbconvert --to=python config_template.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Storage directory\n",
    "DATA_DIR = os.path.join(tempfile.gettempdir(), 'fakenews_data')\n",
    "\n",
    "# Download options.\n",
    "DATA_URL = ('https://storage.googleapis.com/cloud-samples-data/ai-platform/census/data')\n",
    "TRAINING_FILE = 'adult.data.csv'\n",
    "EVAL_FILE = 'adult.test.csv'\n",
    "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
    "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
    "\n",
    "\n",
    "def download(data_dir):\n",
    "    \"\"\"Downloads census data if it is not already present.\n",
    "\n",
    "    Args:\n",
    "    data_dir: directory where we will access/save the census data\n",
    "    \"\"\"\n",
    "    tf.gfile.MakeDirs(data_dir)\n",
    "\n",
    "    training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
    "    if not tf.gfile.Exists(training_file_path):\n",
    "    _download_and_clean_file(training_file_path, TRAINING_URL)\n",
    "\n",
    "    eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
    "    if not tf.gfile.Exists(eval_file_path):\n",
    "    _download_and_clean_file(eval_file_path, EVAL_URL)\n",
    "\n",
    "    return training_file_path, eval_file_path\n",
    "\n",
    "\n",
    "def preprocess(dataframe):\n",
    "  \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "  Args:\n",
    "    dataframe: Pandas dataframe with raw data\n",
    "\n",
    "  Returns:\n",
    "    Dataframe with preprocessed data\n",
    "  \"\"\"\n",
    "  dataframe = dataframe.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "  # Convert integer valued (numeric) columns to floating point\n",
    "  numeric_columns = dataframe.select_dtypes(['int64']).columns\n",
    "  dataframe[numeric_columns] = dataframe[numeric_columns].astype('float32')\n",
    "\n",
    "  # Convert categorical columns to numeric\n",
    "  cat_columns = dataframe.select_dtypes(['object']).columns\n",
    "  dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.astype(\n",
    "    _CATEGORICAL_TYPES[x.name]))\n",
    "  dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.cat.codes)\n",
    "  return dataframe\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "  \"\"\"Loads data into preprocessed (train_x, train_y, eval_y, eval_y) dataframes.\n",
    "\n",
    "  Returns:\n",
    "    A tuple (train_x, train_y, eval_x, eval_y), where train_x and eval_x are\n",
    "    Pandas dataframes with features for training and train_y and eval_y are\n",
    "    numpy arrays with the corresponding labels.\n",
    "  \"\"\"\n",
    "  # Download Census dataset: Training and eval csv files.\n",
    "  training_file_path, eval_file_path = download(DATA_DIR)\n",
    "\n",
    "  # This census data uses the value '?' for missing entries. We use na_values to\n",
    "  # find ? and set it to NaN.\n",
    "  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "  train_df = pd.read_csv(training_file_path, names=_CSV_COLUMNS, na_values='?')\n",
    "  eval_df = pd.read_csv(eval_file_path, names=_CSV_COLUMNS, na_values='?')\n",
    "\n",
    "  train_df = preprocess(train_df)\n",
    "  eval_df = preprocess(eval_df)\n",
    "\n",
    "  # Split train and eval data with labels. The pop method copies and removes\n",
    "  # the label column from the dataframe.\n",
    "  train_x, train_y = train_df, train_df.pop(_LABEL_COLUMN)\n",
    "  eval_x, eval_y = eval_df, eval_df.pop(_LABEL_COLUMN)\n",
    "\n",
    "  # Join train_x and eval_x to normalize on overall means and standard\n",
    "  # deviations. Then separate them again.\n",
    "  all_x = pd.concat([train_x, eval_x], keys=['train', 'eval'])\n",
    "  all_x = standardize(all_x)\n",
    "  train_x, eval_x = all_x.xs('train'), all_x.xs('eval')\n",
    "\n",
    "  # Reshape label columns for use with tf.data.Dataset\n",
    "  train_y = np.asarray(train_y).astype('float32').reshape((-1, 1))\n",
    "  eval_y = np.asarray(eval_y).astype('float32').reshape((-1, 1))\n",
    "\n",
    "  return train_x, train_y, eval_x, eval_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false          742\n",
      "partly true    639\n",
      "true           175\n",
      "Name: labelCode, dtype: int64\n",
      "(1244, 7)\n",
      "(312, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>polarity</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>labelCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>The IOC has announced 3-on-3 basketball will b...</td>\n",
       "      <td></td>\n",
       "      <td>2017-11-06</td>\n",
       "      <td>14812</td>\n",
       "      <td>0</td>\n",
       "      <td>[107744, 115100]</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>\"There are more African American men in prison...</td>\n",
       "      <td>Diego Arene-Morley</td>\n",
       "      <td>2014-11-18</td>\n",
       "      <td>16442</td>\n",
       "      <td>2</td>\n",
       "      <td>[90162, 79665]</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>“We continue to find more evidence that Judge ...</td>\n",
       "      <td>Richard J. Durbin</td>\n",
       "      <td>2018-09-11</td>\n",
       "      <td>15774</td>\n",
       "      <td>1</td>\n",
       "      <td>[60420, 58610, 47489, 29490, 58608]</td>\n",
       "      <td>partly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>\"The Democrats in the Senate and some members ...</td>\n",
       "      <td>John McCain</td>\n",
       "      <td>2008-10-07</td>\n",
       "      <td>1584</td>\n",
       "      <td>1</td>\n",
       "      <td>[10806, 4597]</td>\n",
       "      <td>partly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>\"In Texas, we teach both creationism and evolu...</td>\n",
       "      <td>Rick Perry</td>\n",
       "      <td>2011-08-18</td>\n",
       "      <td>3898</td>\n",
       "      <td>0</td>\n",
       "      <td>[2191, 92099, 92213, 93900, 93901, 82111]</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence            claimant  \\\n",
       "763   The IOC has announced 3-on-3 basketball will b...                       \n",
       "835   \"There are more African American men in prison...  Diego Arene-Morley   \n",
       "1216  “We continue to find more evidence that Judge ...   Richard J. Durbin   \n",
       "559   \"The Democrats in the Senate and some members ...         John McCain   \n",
       "684   \"In Texas, we teach both creationism and evolu...          Rick Perry   \n",
       "\n",
       "           date     id  polarity                           related_articles  \\\n",
       "763  2017-11-06  14812         0                           [107744, 115100]   \n",
       "835  2014-11-18  16442         2                             [90162, 79665]   \n",
       "1216 2018-09-11  15774         1        [60420, 58610, 47489, 29490, 58608]   \n",
       "559  2008-10-07   1584         1                              [10806, 4597]   \n",
       "684  2011-08-18   3898         0  [2191, 92099, 92213, 93900, 93901, 82111]   \n",
       "\n",
       "        labelCode  \n",
       "763         false  \n",
       "835          true  \n",
       "1216  partly true  \n",
       "559   partly true  \n",
       "684         false  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download ():\n",
    "        # Storage directory\n",
    "    import tempfile\n",
    "    DATA_DIR = os.path.join(tempfile.gettempdir(), 'fakenews_data')\n",
    "\n",
    "    # Download options.\n",
    "    # https://storage.cloud.google.com/[BUCKET_NAME]/[OBJECT_NAME]\n",
    "    DATA_URL = ('gs://%s/data' % BUCKET_NAME)\n",
    "    TRAINING_FILE = 'train.json'\n",
    "    TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
    "\n",
    "    print(TRAINING_URL)\n",
    "\n",
    "    if (tf.io.gfile.exists(TRAINING_URL)):\n",
    "        print(TRAINING_URL)\n",
    "\n",
    "    tf.io.gfile.copy(\n",
    "        TRAINING_URL,\n",
    "        'data/',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    with tf.io.gfile.GFile(TRAINING_URL, \"rb\") as file:\n",
    "        train_df = pd.read_json (file.read())\n",
    "        \n",
    "    return train_df\n",
    "\n",
    "def load_data():\n",
    "    train_df = download()\n",
    "\n",
    "    train_df = train_df.sample(frac=.10).reset_index(drop=True)\n",
    "\n",
    "    labels = {0:'false', 1:'partly true', 2:'true'}\n",
    "\n",
    "    def label(x):\n",
    "        return labels[x]\n",
    "\n",
    "    train_df['labelCode'] = train_df.label.apply(label)\n",
    "\n",
    "    print(train_df.labelCode.value_counts())\n",
    "    train_df.labelCode.value_counts().plot(kind='bar')\n",
    "\n",
    "    train_df.rename(columns={\"claim\": \"sentence\", \"label\": \"polarity\"}, inplace=True)\n",
    "\n",
    "    train_df.shape\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(train_df, test_size = 0.2, random_state = 42)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = get_train_test_def()\n",
    "    \n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Koch Industries paid the legal fees of George Zimmerman.'] (1244, 1)\n",
      "0\n",
      "(312, 1)\n",
      "(1244, 3) [1. 0. 0.]\n",
      "(312, 3) [0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "max_seq_length = 256\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "train_label = train_df['polarity'].tolist()\n",
    "encoder.fit(train_label)\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "\n",
    "def get\n",
    "\n",
    "    # Create datasets (Only take up to max_seq_length words for memory)\n",
    "    text = train_df['sentence'].tolist()\n",
    "    text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
    "    text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "    label = train_df['polarity'].tolist()\n",
    "    print(train_text[0], train_text.shape)\n",
    "    print(train_label[0])\n",
    "\n",
    "    train_label = encoder.fit_transform(train_label)\n",
    "    train_label = np_utils.to_categorical(train_label)\n",
    "    print(train_label.shape, train_label[0])\n",
    "\n",
    "    # Convert data to InputExample format\n",
    "    train_examples = convert_text_to_examples(train_text, train_label)\n",
    "\n",
    "    # Convert to features\n",
    "    (train_input_ids, train_input_masks, \n",
    "     train_segment_ids, train_labels) = convert_examples_to_features(train_examples, \n",
    "                                                                     max_seq_length=max_seq_length)\n",
    "\n",
    "    \n",
    "    print('train_input_ids', train_input_ids.shape)\n",
    "    print('train_input_masks', train_input_masks.shape)\n",
    "    print('train_segment_ids', train_segment_ids.shape)\n",
    "    print('train_labels', train_labels.shape)\n",
    "\n",
    "def get_test_inputs():\n",
    "    test_text = test_df['sentence'].tolist()\n",
    "    test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
    "    test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "    test_label = test_df['polarity'].tolist()\n",
    "    print(test_text.shape)\n",
    "\n",
    "    test_label = encoder.fit_transform(test_label)\n",
    "    test_label = np_utils.to_categorical(test_label)\n",
    "    print(test_label.shape, test_label[0])\n",
    "\n",
    "    # Convert data to InputExample format\n",
    "    test_examples = convert_text_to_examples(test_text, test_label)\n",
    "\n",
    "    \n",
    "    (test_input_ids, test_input_masks, \n",
    "     test_segment_ids, test_labels) = convert_examples_to_features(test_examples, \n",
    "                                                                   max_seq_length=max_seq_length)\n",
    "\n",
    "    print('test_input_ids', test_input_ids.shape)\n",
    "    print('test_input_masks', test_input_masks.shape)\n",
    "    print('test_segment_ids', test_segment_ids.shape)\n",
    "    print('test_labels', test_labels.shape)\n",
    "    \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "Next, tokenize our text to create `input_ids`, `input_masks`, and `segment_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids   = [0] * max_seq_length\n",
    "        input_mask  = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label       = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "#         np.array(labels).reshape(-1, 1),\n",
    "        np.array(labels),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|██████████| 1244/1244 [00:00<00:00, 2685.04it/s]\n",
      "Converting examples to features: 100%|██████████| 312/312 [00:00<00:00, 2543.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input_ids (1244, 256)\n",
      "train_input_masks (1244, 256)\n",
      "train_segment_ids (1244, 256)\n",
      "train_labels (1244, 3)\n",
      "test_input_ids (312, 256)\n",
      "test_input_masks (312, 256)\n",
      "test_segment_ids (312, 256)\n",
      "test_labels (312, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input_ids [  101 26914  2378 11300  2522  1011  6485  1037  2857  2740  2729  3021\n",
      "  2008  2018  2019  3265 11405  1012   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "train_input_masks [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_segment_ids [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_labels [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('train_input_ids', train_input_ids[1])\n",
    "print('train_input_masks', train_input_masks[1])\n",
    "print('train_segment_ids', train_segment_ids[1])\n",
    "print('train_labels', train_labels[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
