FDA and Clinical Drug Trials: A Short History
Suzanne White Junod, Ph.D. 1

The function of the controlled clinical trial is not the "discovery" of a new drug or therapy. Discoveries are made in the animal laboratory, by chance observation, or at the bedside by an acute clinician. The function of the formal controlled clinical trial is to separate the relative handful of discoveries which prove to be true advances in therapy from a legion of false leads and unverifiable clinical impressions, and to delineate in a scientific way the extent of and the limitations which attend the effectiveness of drugs.

William Thomas Beaver 2

Overview

The U.S. Food and Drug Administration has evolved as one of the world's foremost institutional authorities for conducting and evaluating controlled clinical drug trials.

Ancient civilizations relied on medical observation to identify herbs, drugs and therapies that worked, and those that did not. Beginning in the early twentieth century, therapeutic reformers in the United States and in other places began to develop the concept of the "well-controlled" therapeutic drug trial. This concept, included, for example, laboratory analysis followed by clinical study. As medical historians have pointed out, however, these early reformers' therapeutic vision often far exceeded their clinical and experimental grasp. 3 In 1938, a newly enacted U.S. Food, Drug, and Cosmetic Act subjected new drugs to pre-market safety evaluation for the first time. This required FDA regulators to review both pre-clinical and clinical test results for new drugs. Although the law did not specify the kinds of tests that were required for approval, the new authority allowed drug officials to block the marketing of a new drug formally or delay it by requiring additional data. The act also gave regulators limited powers of negotiation over scientific study and approval requirements with the pharmaceutical industry and the medical profession. A worldwide drug disaster in 1961 resulted in the enactment of the 1962 Drug Amendments, which explicitly stated that the FDA would rely on scientific testing and that new drug approvals would be based not only upon proof of safety, but also on "substantial evidence" of a drug's efficacy [i.e. the impact of a drug in a clinical trial setting]. Increasingly, responsibility for testing standards previously established as voluntary by the American Medical Association's (AMA) Council on Drugs, the U.S. Pharmacopeia and the National Formulary were taken up by the FDA. Since 1962, FDA has overseen substantial refinements to the broad legal requirement that post-1962 new drugs be approved on the basis of "adequate and well-controlled" studies. 4

Medical Observation As Precursor to Clinical Trials

Clinical trials are prospective, organized, systematic exposures of patients to an intervention of some kind (drug, surgical procedure, dietary change). The earliest recorded therapeutic investigations, however, lacked the rigor of a modern clinical trial. Based largely on observations and tested through time by trial and error, ancient medicine such as that practiced by the Egyptians, Babylonians, and Hebrews was closely allied with religion. Nonetheless, some of these early medical investigations did yield some important successes in fields such as minor surgery and orthopedics. The Hebrews, in particular, excelled in public hygiene, but even their public health strictures, so effective in preventing epidemic disease, were observational and experiential rather than experimental. 5

The Babylonians reportedly exhibited their sick in a public place so that onlookers could freely offer their therapeutic advice based on previous and personal experience. 6 The first mention of a paid experimental subject came from Diarist Samuel Pepys who documented an experiment involving a paid subject in a diary entry for November 21, 1667. He noted that the local college had hired a "poor and debauched man" to have some sheep blood "let into his body." Although there had been plenty of consternation beforehand, the man apparently suffered no ill effects.

One of the most memorable successes from an early but earnest clinical trial was actually more of an anomaly rather than a harbinger of great progress in medical experimentation. British naval surgeon James Lind (1716-1794), who had learned of the death of three quarters of a ships' crew during a long voyage around the world, planned a comparative trial of several popularly suggested "cures" for the scurvy on his next voyage. Twelve men with similar cases of scurvy ate a common diet and slept together. Six pairs, however, were given different "treatments" for their malady. Two were given a quart of cider daily; two an "elixir;" two seawater; two a remedy suggested by the ship's surgeon (horseradish, mustard and garlic); two vinegar; and the final two were given "oranges and lemons" daily. One man who received the oranges and lemons recovered within six days, while the other recovered sufficiently that he "was appointed nurse to the rest of the sick." At first Lind questioned his own experimental results, but by the time he published them (1753 and 1757) they were recognized as important. Nonetheless, the British Navy did not supply citrus to its ships until 1795. 7

Although simple observation may provide a starting point for medical study, however, experience has shown that it is rarely efficient at advancing medical knowledge. As one early proponent of planned experimentation in the form of clinical trials remarked, "when we are reduced to [mere] observation, science crawls." 8 A modern drug regulator is more explicit, acknowledging that modern retrospective [studies], epidemiologic analyses, and astute observations are all instructive. Although clinical trials are not the only way to find things out, the clinical trial is unique. "It is under the investigator's control, subject not to data availability or chance but to his ability to ask good questions and design means of answering them." 9

Evolution of Clinical Trial Concept in America

According to medical historian Harry Marks, the modern controlled clinical trial is largely an American invention as statistically-based clinical trials became a critically important part of evidence-based medicine in the U.S. following WWII. 10 Certainly clinical trials in this country have evolved in pursuit of a larger therapeutic goal -- to see that the physicians use the best possible therapies available. It is interesting to note that in the late 19th century, U.S. antivivisectionists protested against the use of human beings as subjects in medical experiments. In their quest to protect animals, they viewed both animals and human beings as equally vulnerable, and feared that the replacement of the family physician by a "scientist at the bedside" would inspire non-therapeutic experimentation. It was the antivivisectionist and playwright George Bernard Shaw, in fact, who first used the term "human guinea pig." 11

Nonetheless, as early as the late nineteenth and early twentieth century, interest in clinical objectivity grew, spurred on not only by astounding successes in laboratory science and clinical medicine abroad (e.g. discovery of microbes, pasteurization of milk, development of anthrax and rabies vaccines) but because of the sorry state of therapeutics at the time in America. In 1880, patent medicines – a misnomer because nothing but the label and the bottle were actually patented or trademarked – constituted 28% of marketed drugs. By 1900, however, they represented 72% of drug sales and products with inert ingredients were promoted as vigorously, if not more so, than drugs with active ingredients. It was popular to blame both the gullible physician and the ignorant laymen for being equally taken in by the advertising excesses of the era. 12

The American Medical Association (AMA) began to push for federal evaluation of new medical products hoping to make a dent in the patent medicine industry, but it was unsuccessful. In 1905, the AMA formed its own Council on Pharmacy and Chemistry which levied a fee on manufacturers to evaluate their drugs for quality (ingredient testing) and safety. Drugs accepted by the Council could carry the AMA's Seal of Acceptance and only products with the seal had access to the advertising pages of the Journal of the American Medical Association (JAMA). The AMA's Chemical Laboratory tested commercial statements about the composition and purity of drugs in their labs, while the Council on Pharmacy and Chemistry followed up with safety evaluations and rudimentary efficacy evaluations designed to eliminate exaggerated or misleading therapeutic claims. 13 Although the Council eagerly sought evidence that drugs had an effect on the cause or course of a disease, the Seal was awarded to drugs that merely provided symptomatic relief. Although the Council would have liked to rely upon clinical studies to supplement laboratory studies submitted by drug manufacturers, they lacked the necessary funding to support such studies and the AMA did not authorize the Council to require them. Instead of relying on the anecdotal information provided by private practitioners, however, the Council relied heavily on the opinions and recommendations of Council members who were well-respected medical specialists and scientists, a progressive practice for the era. Once their evaluations became a regular feature in the Journal of the American Medical Association (JAMA) the Council began to make inroads against the commercialism that physicians had felt were "debauching" medical journals and "tainting" medical textbooks. The AMA's drug certification program remained in place until 1955.

Clinical Trials and the 1906 Pure Food and Drugs Act

While the AMA Council on Pharmacy and Chemistry held out a carrot of certification to ethical drug products that met their standards, the first federal food and drug statute, the 1906 Pure Food and Drugs Act, wielded little in the way of a stick. The AMA had been unsuccessful in getting any kind of drug review in the new law and the statute merely provided a legal definition for the terms "adulterated" and "misbranded" as they related to both food and drug products and prescribed legal penalties for each offense. The law did empower the Bureau of Chemistry (forerunner of the U.S. Food and Drug Administration) to seize adulterated and misbranded products that moved in interstate commerce, but it simply adopted the drug standards as published in the U.S. Pharmacopeia and the National Formulary. The law also prohibited "false and misleading" statements on product labels. In the case of drugs, the law listed eleven so-called "dangerous ingredients" including opium (and its derivatives) and alcohol which, if they were present in the product, had to be listed on the drug label. This listing requirement alone inspired many manufacturers to abandon use of many dangerous ingredients following passage of the 1906 Act. But efforts to prohibit false therapeutic claims on drug labels were defeated both by the Supreme Court and the U.S. Congress.

During the 1920's, 30's and 40's medical researchers began to conduct "cooperative investigations" designed to overcome errors attributed to individual observers working in relative isolation and replace them with standardized evaluations of therapeutic research in hundreds of patients. 14 Therapeutic experimentation, however, did not begin to gain a true foothold in modern medicine until the U.S. legal system stopped equating experimentation with medical malpractice. As late as 1934, state courts seemed to uphold traditional views that the doctor was bound to act within accepted methods of clinical practice and that patients had not consented for their physician to deviate from these methods. 15 In a landmark state Supreme Court decision in 1935, however, the state of Michigan seemed to recognize and authorize controlled clinical investigations as a part of medical practice without subjecting the researcher to strict liability (without fault) for any injury so long as the patient consented to the experiment and it did not "vary too radically" from accepted methods of procedure. 16 In particular, the Michigan Supreme Court accepted that experimentation was necessary not just to treat the individual, but also to help medicine progress. "We recognize," noted the Court, "the fact that if the general practice of medicine and surgery is to progress, there must be a certain amount of experimentation carried on."

By 1937, it had become clear to regulators and to an increasing number of outside organizations, including the AMA, that the original 1906 "Wiley" Act had become outdated. Breakthrough drugs such as the first sulfa drug, sulfanilamide, new drugs including amphetamines and barbiturates, and biologics such as insulin were coming onto the market and beginning to transform medicine entirely. Clinical trials and human experimentation were becoming increasingly more important in medical research. Moreover, turn-of-the-century patent medicines with inert ingredients and quirky but quaint labels were becoming a true public health danger when patients relied on them rather than seeking out effective new therapies. The case of Banbar, in particular, convinced regulators early in the 1930's that the 1906 law's recognition of the rights of proprietors was becoming an increasing impediment to efforts to insure drug safety.

Soon after the 1906 Act had been enacted, a dispute arose over the meaning and enforcement of the drug labeling provisions of the law. The Supreme Court ruled in U.S. v. Johnson in 1911, that the new law did not prohibit false therapeutic claims – the product involved was labeled Dr. Johnson's Cure for Cancer – it just prohibited "false and misleading" label claims regarding the ingredients or identity of the drug. In 1912, Congress quickly enacted the Sherley Amendment, a compromise that merely prohibited false therapeutic claims "intended to defraud" the consumer. Proving that a proprietor knew that his drug was worthless in order to demonstrate fraud under the statute, however, could be a daunting task. To cite a single example: an old patent medicine maker created a "cure" for diabetes which he marketed as Banbar. Its active ingredients included milk sugar and equisetum (horsetail). The product was particularly dangerous since diabetics were rejecting insulin injections in favor of Banbar (the hormone insulin had been isolated in 1922 and was a lifesaving therapy for diabetics). FDA seized the product in the mid-1930s, charging the proprietor with fraud under the Sherley Amendment. In his defense, the proprietor submitted testimonial letters written to him thanking him for the product. His lawyer argued that it was obvious, since these sincere people took the trouble to write him and thank him, that he had no idea that the product might not be effective much less dangerous. Government officials selected a representative group of testimonial letters and matched them side-by-side with death certificates from the same individuals indicating that they had died from diabetes. Although the public health threat was obvious, the court ruled that the proprietor had not intended to defraud his customers and the product remained on the market until Congress enacted a new food and drug statute without this so-called "fraud joker" in 1938. Banbar, in particular, gave drug regulators their first direct experience interpreting drug data obtained not from direct clinical trials, but from both uncontrolled trials and "historical" data, one of three types of clinical trial data eventually recognized as acceptable under law in 1970. 17

Most consumers were unaware of Banbar, but in 1937, a broader drug disaster did capture public attention and first drew the federal government into playing a limited, but soon growing role in the evaluation of new drugs, including the conduct of clinical trials for new drugs. In 1937 a drug company developed a liquid preparation of the first "wonder drug" sulfanilamide, used to fight streptococcal infections (i.e. strep throat). The product was not tested in animals or humans prior to marketing. The solvent used to suspend the active drug, diethylene glycol, was a poison (chemically related to anti-freeze). It required the entire field force of the FDA to retrieve all available bottles of Elixir Sulfanilamide when the company's own recall efforts proved inadequate to the task. FDA officials soon discovered that adequate records had not been kept by either physicians or pharmacists documenting prescriptions written and filled for the poisonous product. FDA, however, was only empowered to act against the deadly product because it was misbranded – it contained no alcohol whereas the term "elixir" implied that it did contain alcohol.

Clinical Trials and the 1938 Food, Drug, and Cosmetic Act

Congress reacted to the tragedy, which killed over 100 people, by enacting a new federal food and drug statute, the 1938 Food, Drug, and Cosmetic Act. A new provision in the act-- requiring drug sponsors to submit safety data to FDA officials for evaluation prior to marketing -- appeared with relatively little discussion following on the heels of the Elixir Sulfanilamide disaster. "Instead of going to market based on their own assessment of the drug, sponsors had to notify the FDA of their intent to market the drug by submitting an NDA (New Drug Application)," explains Dr. Robert Temple, currently head of FDA's Office of Medical Policy. Although the new law did not specify any particular testing method(s), the law did require that drugs be studied by "adequate tests by all methods reasonably applicable to show whether or not the drug is safe."Sponsors were required to demonstrate to FDA that they had carried out all reasonably applicable studies to demonstrate safety and that the drug was "safe for use under the conditions prescribed, recommended or suggested in the proposed labeling thereof." 18 In the future, FDA could use these new tools not only to ban Banbar, but to try and prevent drug disasters rather than merely react to them.

Under the law, there was no true requirement for FDA "approval" or "clearance" of a new drug. Rather, it was presumed that most drugs would be marketed and therefore the default position was "approval." 19 Under the 1938 Act, the government had sixty days (could be extended to 180 days) to complete its safety evaluation. Form 356, the New Drug Application (NDA), required information about all clinical investigations, a full list of the drug's components and composition, methods of manufacture including facilities and controls, and copies of both the packaging and labeling of the new drug. If a company had not received a regulatory response at the end of 60 days it could proceed with marketing its new drug.

Regulators adopted many of the standards and rules of evidence first advocated by turn-of-the-century therapeutic reformers. 20 Laboratory analysis akin to that originally conducted by the AMA's Chemical Laboratory initially screened most new drugs, companies were required to conduct safety studies, and an increasing number of drugs would soon be studied in the kind of clinical (cooperative) drug trials that the AMA's Council on Pharmacy and Chemistry had advocated, but not conducted, earlier in the century. 21 Animal studies were not required under the 1938 Act to precede human drug trials, but such studies, including animal autopsies, could be requested by regulators as part of the agency's drug safety review. FDA also began to employ the practice, similar to that of the Council, of consulting expert academic specialists, often before making a final decision on drug approvals. 22

FDA's statutory authority over products increased as a result of egregious public health disasters, but the associated scientific methodology to evaluate safety and efficacy did not accelerate in tandem. Regulatory work under the new drug safety provisions of the Act was fairly limited, although the new law did sanction factory inspections for the first time and officials were able to eliminate many worthless products submitted for approval to treat serious diseases (i.e. cancer and diabetes) by holding them to be "unsafe" under the statute. 23 Regulators could deny an application if the sponsor's drug application did not include "adequate tests by all methods reasonably applicable to show whether or not such drug is safe for use under the conditions prescribed, recommended or suggested in the proposed labeling thereof." 24 Occasionally, in interpreting this provision, agency officials recommended labeling changes, including warnings, to sponsors and to the U.S.P., but FDA itself lacked authority under the 1938 Act to determine the text and layout of drug labels. 25 Larger efforts to improve drug testing, prescribing patterns, and patient use and compliance, however, were left to the practice of medicine and medicine's scientific and professional authorities.

Although FDA had authority under the 1938 Act to establish rules governing the use of investigational drugs, FDA did not employ this authority to regulate clinical trials and clinical trial methodology until 1961. 26 Even though physicians at elite university clinics and members from the AMA Council on Pharmacy and Chemistry all agreed on the importance of standardized drug testing through clinical trials, FDA did not have the authority to require them under the 1938 statute. 27 FDA scientists, however, did begin to exert some influence on the conduct of clinical trials and move in the direction of standardization on the eve of WWII, when they published an article in JAMA on experimental design, proper clinical trial methods, and methods of data analysis. 28 Their article, however, was published as a Report under the auspices of the AMA's Council on Pharmacy and Chemistry and was accompanied by a disclaimer to the effect that the "outline" presented in the report was "offered as an objective, a pattern, and not a regulation." During WWII, the agency actively promoted drug testing standards in the face of increased wartime expenditures for drug trials designed to answer important questions about the safety and use of many new drugs for the war effort. 29 An important breakthrough in clinical trial design followed from the shortages of a new drug, streptomycin, shortly after the war.

Following war trials of penicillin, British epidemiologist and biostatistician, A. Bradford Hill, was faced with the task of testing a promising antibiotic, streptomycin, against tuberculosis. Researchers in the United States studying the same drug had ample supplies and led to more effective treatment for patient subjects but produced less conclusive clinical trial data. 30 Hill and his colleagues, however, were faced with a severe shortage of the streptomycin drug they were studying. In post-war Britain, the central government could not afford to purchase more of the drug. Scarcity and expense, therefore, justified their decision to formally but randomly assign patients to control groups and treatment groups. This eliminated a well-known form of treatment "bias" in which physicians are known to select their healthier patients for experimental treatment leaving sicker patients in the control group. Hill's study was a true randomized study. It was not, however, "double blinded" – another way of insuring the objectivity of a trial by neutralizing the power of "suggestion."

In a double-blind clinical drug study, trials are designed in such a way that neither the patient nor the researcher knows who is receiving the treatment drug. 31 In Hill's study, streptomycin required injection, and the researchers did not wish to use inert injections. However the lack of true double-blinding had little impact on the results, since Hill was able to show conclusively that streptomycin could cure tuberculosis. When the results of his study were published in 1948, Hill's use of concurrent controls (randomized, controlled) was praised as having ushered in "a new era of medicine." 32

Hill and his North American colleagues, including Harry Gold at the Cornell Medical School, began to map out general criteria for drug testing and specify stages through which drug development should proceed. Patients were to be selected through formal criteria and then randomly separated into treatment and control groups; trials were to be double-blinded and employ objective diagnostic technologies; and drug doses were to be administered according to a fixed schedule, while patient observations were to be charted at uniform intervals. Their success set the stage for the subsequent development of more sophisticated clinical trial designs while professional collaborations allowed statisticians to increasingly dominate the conduct of clinical trials in the U.S. 33 Nonetheless, one expert estimated in 1951 that 45% of clinical trials had no control groups. 34

After WWII, medical research increased exponentially in the United States. In 1950, funding for medical and scientific research was $161 million dollars. By 1968, this figure had grown to over $2.5 billion. 35 The National Institutes of Health (NIH) opened its Clinical Center in Bethesda, MD as a research hospital in 1952, and NIH's extramural, peer-reviewed research grant system soon supported biomedical and clinical research projects at institutions around the country. Centrally planned clinical research projects, including cooperative trials, were soon eclipsed as grants supported the work of individual medical investigators, many of whom designed and conducted their own clinical trials in collaboration with other colleagues. Ethical concerns about the protection of research subjects further complicated clinical trial design, post-war, particularly following reports of the gross medical abuses carried out on Nazi prisoners of war. 36 Ethical debates over methodology often centered around questions concerning when it was appropriate to use placebo controlled trials and when it was preferable to compare active treatments in evaluating new therapies. The NIH Clinical Center adopted a policy that placed much of the responsibility for safeguarding human subjects of biomedical research with principal investigators. Research involving normal human volunteers was to be formally reviewed by panels of scientists, but there was virtually no discussion about any potential role for the federal government in regulating medical research. Meanwhile, both NIH and FDA gave clinical investigators wide latitude in the pursuit of their research objectives. 37

Sulfa drugs and antibiotics, among other therapies for acute diseases, had provided important experience in evaluating new drugs, but increasingly after WWII, investigators and regulatory officials began to rely on increasingly sophisticated trial designs to study effectiveness in whole new classes of drugs for chronic, rather than acute conditions. Blood pressure and anti-arrhythmic drugs (1950s/60s), drugs for tuberculosis, cancer, heart disease, and the oral contraceptives (1960) were all approved using new and increasingly advanced trial methodology involving assessment of data from sometimes tens of thousands of patients. Statisticians insisted on uniform selection criteria for patients in clinical trials, separate treatment and control groups, uniform dosing regimens, and utilized objective evidence from laboratory tests such as blood and urine tests made both before and after treatment. With the aid of a new science of biostatistics, both regulators and regulated industry began to understand, appreciate, and interpret many nuanced components of trial design and their effect on the interpretation of data. 38 Although several kinds of randomized controlled trial methodologies can be useful to researchers and regulators, ultimately, it was the randomized, double-blinded, placebo controlled experiment which became the standard by which most other experimental methods were judged, and it has often subsequently been referred to as the "gold" standard for clinical trial methodology. In situations in which using a placebo seemed unethical, positive (treatment) groups rather than placebo groups were employed and regulators had to learn how to interpret the data stemming from these trials as well, a formidable problem in many cases. 39

The Kefauver Hearings and Drug Critics

In the early 1950s the AMA discontinued many of its drug study activities. It closed its microbiological laboratory used to test new drugs (successor to the Chemical Laboratory) and discontinued its Seal of Acceptance program. Since only drugs that had the Seal could advertise in the pages of AMA periodicals, the discontinuation of this program opened the door for an explosion of advertising (and advertising revenue) in JAMA and other AMA publications. AMA discontinued its inspection of drug plants, its efforts to exert some control over generic drug names, and even a campaign it had instituted to explain and encourage physicians to prescribe using generic names rather than brand names. 40 In their place, the AMA initiated a registry for reporting adverse drug reactions, although it had no mechanism to enforce data collection. 41

Beginning in 1958, hearings on the drug industry held by Senator Estes Kefauver (D-Tennessee) focused unanticipated attention on the quality of drug company sponsored clinical drug research. In particular, the hearings drew attention to the poor state of clinical trial research as it had been conducted (or failed to be conducted) under the 1938 statute. Kefauver announced his hearings on the drug industry – its products and its profitability -- after he and his staff had obtained evidence documenting the high markups and exorbitant profit margins that had become evident on prescription drugs, beginning with antibiotics. Yet the hearings soon turned to other topics as the industry tried to defend its profits by asserting the high cost of research, including the costs of conducting clinical trials. As popular with consumers as they proved unpopular with the pharmaceutical industry, these hearings generated important evidence documenting the frequently sorry state of drug testing and advertising as well as the competitive pressures within the industry which supported such practices. Able testimony was offered documenting many poor clinical studies done in support of the marketing of many mediocre drugs. Dr. Louis Lasagna, an expert in clinical pharmacology, testified that it was "shocking that experimental drugs are subject to no FDA regulation of any sort before patients receive them…It is reprehensible for man to be the first experimental animal on which toxicity tests are done, simply because bypassing toxicity tests in laboratory animals saves time and money." 42 At one point in the hearings a former medical director at Squibb testified that the industry was always pointing out the high costs of research and the fact that so many products failed in the course of research to justify its markups and profit margins. "This," he agreed, "was true, since it is the very essence of research." The problem, he quipped, lay in the fact that "they market so many of their failures." 43 Most new drug products, experts testified, were not improvements over old ones, and most were marketed before clinical studies were published. Many new drugs, in fact, were combinations of older drugs, with or without modification, which gained extended patent life (and profitability) in combination. Adequately controlled comparisons of drugs, Lasagna testified, were "almost impossible to find." 44

Years later, FDA's Chief Counsel William Goodrich recalled that during the Kefauver hearings the pharmaceutical industry "stepped right into the bear trap" when it tried to defend itself by touting the high costs of research and development for new drugs.

That just focused attention on these various phases of new drug development and promotion…first of all, was it really all that expensive? Were they really doing all that kind of research? And anyone who had looked at any of the New Drug Applications knew, as I knew, that that was all baloney, and what they were saying to us in those early days was essentially a bunch of testimonials. The way drugs were investigated--a physician from the company would go out in the community with some samples and say to the doctor, "I've got this new drug for so-and-so. Here's some samples. Try it out and let us know how you like it." And they would get back a letter from him: "I tried it out on eight patients and they all got along fine." That's the kind of stuff that was coming in for the science. Of course, that was completely unsatisfactory, and as soon as people focused on that, that raised the problem. 45

By the 1960s, following another drug crisis in 1962, there was a growing recognition of the importance of clinical trials in new drug development as well as in clinical medicine. Pharmacologists and medical researchers as well as officials at government agencies such as the Veteran's Administration and the National Institutes of Health knew more about the conduct of good clinical trials than did the FDA at that time. This changed rapidly, however, beginning with a drug crisis in 1962. Following a pattern first seen in the elixir sulfanilamide crisis which led to changes in U.S. drug regulation in 1938, a similar crisis in 1962 spurred even more widespread changes, both in the U.S. and around the world. In 1961, a popular drug in Europe, a hypnotic known as thalidomide, was discovered to cause severe birth defects and even death in babies when their mothers took the drug early in their pregnancies. Because of the concerns of FDA drug reviewer Dr. Frances Kelsey, the drug was never approved for sale in the U.S. Nonetheless, the drug sponsor had sent samples of the drug to thousands of U.S. doctors who gave the samples to their patients without telling them that the drug was an experimental one, making their patients the unwitting subjects of human drug experimentation. It is believed that there were more than a dozen thalidomide babies born in the United States as a result of this unauthorized "sample" program. As a result of the worldwide thalidomide disaster, countries around the world, including the United States, updated their drug regulatory systems and statutes. "In next to no time," recalled Frances Kelsey, "the fighting over the new drug laws that had been going on for five or six years suddenly melted away, and the 1962 amendments were passed almost immediately and unanimously." 46

The IND Process and Clinical Trial Regulation

Prior to the law's final passage, regulations began to address known problems in the use of clinical trials by the drug industry, indicating that FDA felt more confident in its authority to regulate them, even under the old 1938 statute. 47 New regulations prohibited testing a drug in humans until preclinical studies could predict that the drug could be given safely to people. 48 The 1962 [Kefauver-Harris] Drug Amendments and the 1963 investigational drug regulations themselves introduced many new procedures that strengthened control over investigational new drugs in the United States. 49 One of the most significant was a system of pre-clinical trial notification and approval designed to provide enough information to regulators to demonstrate that it was safe to conduct clinical trials. Under this new system, company drug sponsors were required to file a "notice of claimed investigational exemption for a new drug." The "notice" was actually a package of materials that a company submitted to FDA for approval prior to starting human trials. The acronym IND (Investigational New Drug) was coined to parallel the acronym NDA (New Drug Application). 50 Technically, an IND is an exemption from the normal pre-marketing requirements for a new drug – namely, the submission and approval of an NDA. 51 An approved IND application allows investigators to proceed with new drug trials for a drug under development. The information collected under an IND may later become a part of an NDA submission if the systematic drug tests set up to test the drug are successful. IND's are also required when a sponsor wishes to restudy a previously approved drug in order to gather data in support of significant labeling changes, advertising changes, changes in route of administration or dose, or any other change that might alter the risk/benefit equation upon which the original approval was based. The IND regulations also led FDA to define more clearly through regulation the "phase" process of drug testing involved in the regulatory approval of a new drug in the 1963 regulations. 52

An IND submission 1) alerts regulators to a sponsor's intent to begin clinical studies in the United States 2) provides the preliminary animal toxicity data indicating it is reasonably safe to administer the drug to humans 3) provides information about the manufacturing process for the new drug 4) provides chemistry background material 5) describes the initial clinical study being proposed, focusing on its safety measures (who is conducting the trials, their qualifications and facilities; and the type of study population involved – volunteers, sick patients, prisoners, women, men, children, etc.) and 6) provides assurance than an IRB (Institutional Review Board) will approve the study protocol before the study begins. In addition to the IND submission itself, every investigator participating in the study must sign a form, maintained by the sponsor, indicating their qualifications, the location of the research facility where the study will be conducted, and the name of the IRB responsible for reviewing and approving the study protocol. Investigators must sign commitments to

conduct the clinical study in accordance with the IRB approved protocol personally conduct or supervise the conduct of the investigation inform potential subjects that the drugs are being used for investigational purposes and report to the sponsor adverse events that occur in the course of the investigation.

Efficacy Under the 1962 Drug Amendments

A new and key provision in the 1962 amendments was the requirement that, in addition to the pre-market demonstrations of safety already required under the 1938 Act, future new drugs would also have to be demonstrated "efficacious" prior to marketing. This provision required controlled trials that could indeed support claims of efficacy. The 60 day approval "default" under the 1938 Act was removed. New drugs had to have positive and specific, and increasingly detailed approval from FDA to go to market and FDA was given the authority to set standards for every stage of drug testing from laboratory to clinic. In addition, FDA could require market withdrawals for the first time and establish "Good Manufacturing Practices (GMP's)" to govern drug manufacturing. In order to prevent another "thalidomide disaster," Congress inserted language in the 1962 Drug Amendments requiring that investigators maintain personal supervision over clinical investigations and agree not to give the drug to other investigators. Senator Jacob Javits (D-New York) was particularly concerned about the fact that so many people had taken thalidomide without knowing that it was an experimental drug. Even many doctors that FDA had surveyed had been confused as to the status of the drug at the time they gave it to their patients. 53 Javits sponsored what became a very important provision of the law itself: the requirement that informed consent be obtained from all research study subjects so that patients would have to be specifically informed if a drug they were being given or prescribed was "experimental," something that had not happened in the case of thalidomide.

The legal language employed in the statute, which laid out the criteria that would be used in assessing efficacy in support of a new drug approval, was not particularly stringent. The law required that there be "substantial evidence" that the drug "will have the effect it purports or is represented to have under the conditions of use prescribed, recommended, or suggested in the proposed labeling." Lawyers have concluded that Congress could have established a more stringent drug approval process simply by using stronger legal terminology. The fact that terms such as "preponderance of evidence" or "evidence beyond a reasonable doubt" were not used indicates that Congress did not intend to set the bar for efficacious new drug approvals too high. 54 New drugs did not have to be superior to other drugs on the market nor did "substantial evidence" mean evidence "so strong as to convince everyone." 55

The strength in the statutory language, however, came not from the evidentiary requirements but from a last minute-compromise over study methods. 56 Sponsors were only required to provide "substantial evidence" of effectiveness, but that evidence had to be based on "adequate and well-controlled studies," i.e. clinical trials. Without defining either "adequate" or "well-controlled," the law paved the way for experts in the field to establish the criteria that would define both terms under the new statute. Although the law did not define a well-controlled study, testimony before Congress made it clear that it included, as a minimum, the use of control groups, random allocation of patients to control and therapeutic groups, and techniques to minimize bias including standardized criteria for judging effectiveness. 57 A poorly designed trial, it was argued, not only wasted resources, but it unnecessarily put patients at risk.

Clinical Trial Regulations of 1970 and the DESI Process

Over the next eight years FDA worked diligently to implement the 1962 drug amendments. In the early years after passage of the 1962 amendments, sponsors were more or less "on their own" with little guidance from FDA about what would be acceptable except in the form of an NDA non-approval letter which did explain why the sponsors' submission was considered inadequate. Concerns that FDA might become overly "vested" in the development of a commercial drug product led to an abundance of caution in agency/sponsor interactions. According to one official, "There was, in fact, explicit concern that too much participation by FDA staff in the development process would leave the Agency unable to be properly neutral and analytical when the resulting data were submitted as part of an NDA." 58 Over the years, however, as Robert Temple notes, FDA has become increasingly involved in the development of specific drug products including the design of clinical trials, "reflecting the view that the public, the industry, and the FDA are poorly served by drug development efforts that are poorly designed or inadequate and that therefore waste resources and delay availability of therapy." 59 In the late twentieth century, Congress itself has even begun mandating meetings between regulators and industry concerning the design and conduct of clinical trials deemed particularly important for any of a number of reasons. 60

Regulatory officials soon began to receive an invaluable education in the conduct of clinical trials as a result of the agency's Drug Efficacy Study (DES). The 1962 Drug Amendments required FDA to re-review all drugs that had been approved under the 1938 Food, Drug, and Cosmetic Act (1938-1962) on the basis of safety alone, this time looking for evidence of efficacy. Examining all pre-1962 NDA's posed a daunting task for FDA so in 1966, FDA contracted with the National Research Council of the National Academy of Sciences to perform the review. 61 Thirty panels of experts reviewed specific drug categories using evidence obtained from FDA, the drug's manufacturer, scientific literature, and the personal expertise of the panel members themselves. Their ratings on each claim for a drug fell into six categories: effective; probably effective; possibly effective, ineffective, effective but, and ineffective as a fixed combination (combination drugs for which there was no substantial reason to believe that each ingredient adds to the effectiveness of the combination.). 62

FDA was challenged to devise a method by which those drugs ruled ineffective could be legally removed from the market along with other "me-too" drugs – drugs with the same essential ingredient profile. FDA's initial legal efforts to remove bioflavonoid drugs and an UpJohn fixed combination drug called Panalba were enjoined by the courts. 63 Faced with the prospect of conducting formal administrative hearings on every drug it proposed to have removed from the market, the agency changed its approach, led by FDA's Director of the Bureau of Medicine (and later Commissioner) Dr. Herbert Ley. Ley supported the drafting, publication and implementation of regulations defining "substantial evidence" leading to a showing of effectiveness under the 1962 Amendments. These "evidence rules" had two separate components but companies wishing an administrative hearing on the proposed withdrawal of their pre-1962 drug would have to meet both criteria. 1) The first formally specified the scientific content of "adequate and well-controlled clinical investigations, including clinical investigations, by experts qualified by scientific training and experience to evaluate the effectiveness of the drug involved," under the 1962 statute. Well-controlled trials did not have to be placebo controlled-- they could have active controls, or even historical controls-- but the regulations stated clearly that "uncontrolled studies are not acceptable evidence to support claims of effectiveness." 64 No hearing would be granted unless there was a "reasonable likelihood" that such evidence would be forthcoming. 65 2) The second required the submission of positive results from at least two clinical studies in order to escape an automatic withdrawal of approval for the drug without a hearing. 66 The courts upheld the agency's new approach and according to Peter Barton Hutt, FDA's Chief Counsel from 1971-1975, no hearings were deemed necessary.

By the end of 1971, FDA had disposed of dozens of requests for hearings on the revocation of NDA's. In no instance had it determined that a manufacturer's supporting data were sufficient to justify a hearing. One explanation of this striking consistency is that the agency's substantial evidence regulations embodied requirements for clinical investigations that few pre-1962 studies could meet. The drugs it initially selected for withdrawal, those evaluated by the NAS-NRC as "ineffective" also presented the easiest targets. But it was becoming obvious that a manufacturer would have to make an overwhelming showing to persuade FDA to expend the time and resources that even one hearing would require. 67

The results of the DES study led to recommendations soon implemented through the Drug Efficacy Study Implementation (DESI), which removed over 1000 ineffective drugs and drug combinations from the marketplace. As part of this process, FDA drug reviewers themselves published hundreds of critiques of the clinical studies that had been submitted for approved new drugs in support of the safety requirements mandated in the 1938 statute. Most of these old studies, recalled Robert Temple, who began work at FDA in 1972, were "inadequate beyond belief." As late as the 1960s and early 1970s he notes, "You would be horrified [at the clinical trial data] submitted to the agency. There was often no protocol at all. There was almost never a statistical plan. Sequential analyses were unheard of. It was a very different world." 68

Positive changes in clinical trial methodology, however, soon began to be evident in new NDA and ANDA submissions. "Everyone," notes Temple, "came to believe that trials should have a prospectively defined and identified endpoint, a real hypothesis and an actual analytical plan." An international, professional organization, the Society for Clinical Trials, was organized in 1978 and began to develop and discuss clinical trial design and the analysis of clinical trials in government as well as industry sponsored clinical trial research. FDA assisted the drug industry during the late 1970s, by collaborating with external advisory committees and conducting FDA-industry workshops in support of the development of nearly 30 drug class clinical guidelines which described in detail the study designs and expected data required for particular therapeutic classes such as drugs for ulcer disease, depression, or angina.

During the AIDS epidemic of the 1980s, regulators were again pushed to consider the essential requirements of a meaningful clinical trial. FDA had created a special class of investigations known as the "Treatment IND" in 1987 in which patients could receive an investigational drug outside the normal "blinded" research setting. 69 Although data from patients under this protocol was still collected, the program was not especially conducive to the treatment of large numbers of patients, especially those desperately sick patients who pushed for access to drugs at their earliest stages of development. 70

In 1985 regulations recognized what had already become a central tenet of modern drug evaluation by formalizing the requirement that approvals be based on an "integrated summary of all available information about the safety of a drug product." 71 Congress itself mandated in 1988 that each AIDS drug IND must be publicly disclosed in a computer-accessible data base to facilitate access by patients with AIDS, and formally recognized the importance of FDA's Treatment IND program in support of AIDS patients. 72 Although some AIDS organizations requested agency support of "open clinicals" in which a drug sponsor could allow any patient access to ongoing trials with the support of their physicians, FDA refused to allow such easy access. "The more open-ended the design of a clinical trial," noted agency officials, "the less likely the chance the trial will provide answers." 73 Between 1990 and 1992 guidelines were proposed and negotiated, and regulations finally approved by FDA establishing a "parallel track approval" process in which special categories of drugs would be expedited during the review process and a wider group of patients would have access to the drug than under normal procedures. 74

Beginning in the mid 1980s, FDA has focused on improving the analysis of data from clinical trials. One lesson learned from the AIDS epidemic and the concomitant development of clinical trials necessary to test drug products for its treatment is the scientific utility of surrogate endpoints in certain circumstances. Some of this data analysis has been motivated by sponsors' interest in presenting evidence of clinical effectiveness through measurements of biomarkers and evaluation of "surrogate endpoints." Surrogate endpoints measure outcomes that are not clinically valuable by themselves (lowered cholesterol, blood pressure, elevated t-cell counts) but are thought to correspond with improved clinical outcomes (decreased heart disease or stroke, fewer opportunistic infections for AIDS patients). FDA approved the first statin drug, for example, in 1987, based on the surrogate of lowering blood cholesterol. 75 FDA is cautious, however, in accepting surrogates and usually requires continued post-market study to verify and describe continued clinical benefits. In 1992, new regulations for the accelerated approval of new drugs gave the agency explicit authority to rely on a surrogate marker. 76

In 1994, FDA made changes in its policies designed to facilitate women's participation in the earliest phases of clinical drug trials. 77 Most recently, FDA has issued guidelines promoting greater study and better analysis of patient subgroups including drugs in the elderly, separate analysis of trial data for both genders, and pediatric studies as well as dose-response information. 78

The Future

In an era in which health care costs are rising at rates far higher than the rate of inflation and the nation faces the challenge of promoting the health of the "boomer" generation during its retirement years, there have been cries for more comparative drug studies, in part to help contain drug costs. Greater knowledge of genetic science and the ability to conduct more nuanced analyses of drug trial data, including retrospective meta-analyses, have also helped fuel optimism over the future of personalized medicine. In the past the drug industry has concentrated on developing so called "block-buster" drugs. The large scale, randomized clinical trial has been critical in demonstrating the safety and efficacy of these drugs. Many, however, are predicting that the future of medicine points toward developing drugs and diagnostics to treat sub-sets of patients who may respond to one treatment but not another because of genetic and other factors. This has led many to speculate on the future of randomized trials. "The randomized clinical trial is excellent methodology if you want to understand, on average, whether one treatment is better than another treatment," notes John Bridges, assistant professor at Johns Hopkins School of Public Health, "but if we think about a distribution of outcomes, no single person in the health care system is the average." 43 Personalized medicine presents challenges of its own, including increased costs for researchers testing drugs and patients taking them. It seems more likely that better analysis of clinical trial data, already being encouraged by the FDA, and pursued by both researchers and drug sponsors as the first step towards a more personalized perspective on drug development, will be an integral part of the evolution of personalized medicine, while continuing to add to our overall knowledge of the safety and effectiveness profiles of medicines and therapeutics already on the market. The randomized clinical trial is unlikely, in either scenario, to go the way of the dinosaur.

Originally published as "FDA and Clinical Drug Trials: A Short History," in A Quick Guide to Clinical Trials, Madhu Davies and Faiz Kerimani, eds. (Washington: Bioplan, Inc.: 2008), pp. 25-55.