0118 tctapftexas

Email, Holly Eaton, director of professional development and advocacy, Texas Classroom Teachers Association, Jan. 24, 2018

From: Selby, Gardner (CMG-Austin)

Sent: Tuesday, January 23, 2018 3:32 PM

Subject: How Texas ranks for its public schools

Texas school and school employee advocates:

I write because we’re fact-checking a candidate’s claim that Texas ranks 43rd nationally for its public schools. This claim, by Democratic gubernatorial hopeful Andrew White, ties to the 2016 Quality Counts analysis by Education Week, which has since posted updates placing Texas 40th.

For our story, we’re interested in your takes on the Quality Counts approach to ranking public schools.

What are the strengths and weaknesses of this approach?

Are there other state-by-state rankings that have merit?

Any other considerations or recommended resources/experts?

As ever, we count on attributable on-the-record information for our stories. We hope to complete our review Wednesday.

I’d be happy to hear back by phone or email.

Thanks,

g.

Want our fact checks first? Follow us on Twitter.

W. Gardner Selby

Reporter / News

Austin American-Statesman

PolitiFact Texas

10:41 a.m.

Lonnie forwarded your e-mail to me to respond to your request. Here’s what I have to offer on this. Please let me know if you have any questions.

Thanks,

Holly

Holly Eaton Director of Professional Development and Advocacy Texas Classroom Teachers Association

One of the problems with this approach is that it relies, in part, on student performance (on NAEP) without adjusting for student demographics. Using this approach, Texas ranks about average nationally, but when adjusting for student demographics, Texas ranks 2nd in the nation (see chart below). This illustrates the pitfalls of a system that attempts to reduce the complexities of education performance into a single letter grade, and then uses the letter grade to “rank” the states. You have to look at the metrics, the data, and the methodology behind the “grade” to get meaningful information – otherwise, the grade itself can be very misleading.

As noted by the researchers associated with the Urban Institute that produced the above chart, “Raw NAEP scores are unhelpful at best and misleading at worst. Demographic adjustments are never perfect, but they allow for much more meaningful comparisons across states and over time.” https://www.urban.org/urban-wire/how-do-states-really-stack-2015-naep

The researchers, relying on the same methodology they used in a just-released report that same year (2015), “Breaking the Curve. Promises and Pitfalls in Using NAEP Data to Assess the State Role in Student Achievement”, provided an approximation, that, when adjusting for student demographics, Texas ranks 2nd in the nation on the 2015 NAEP, producing stronger academic outcomes for students compared to demographically similar students across the US.

Additionally, the Breaking the Curve report, using demographically adjusted data, found that on the 2013 NAEP, Texas ranked 3rd in the nation.

https://www.urban.org/sites/default/files/publication/72411/2000484-breaking-the-curve-promises-and-pitfalls-of-using-naep-data-to-assess-the-state-role-in-student-achievement.pdf

The report concluded that “ The NAEP data reveal significant variation in average test scores across states, even after taking student characteristics into account. But the relative ranking of states changes substantially when each state’s students are compared with similar students in other states, as opposed to a simple comparison of average scores. This means that interstate comparisons of NAEP scores should not be done without considering the characteristics of each state’s student population.”

“Figure 2 ranks states by the demographically adjusted performance of their students, averaged

across the four tests and reported in months of learning.12 It is important to bear in mind that this is only a relative measure (with the average state arbitrarily assigned a value of zero), not an absolute measure of performance. Examining both the adjusted and unadjusted measures shows that the two measures diverge significantly. The demographic adjustment leads to an average change of nine positions in the 50-state ranking (see table A.1 for both rankings).

The top two states in terms of adjusted student performance are the same two states with the top unadjusted scores: Massachusetts and New Jersey. To be sure, the adjusted performance measures for these states are smaller than their unadjusted measures—my analysis indicates that their students are 6–9 months ahead of their peers in the average state, as compared with the 10–12 months suggested by the raw data.

But the next two spots go to Texas and Florida, respectively, which have average overall

performance, but where student performance significantly exceeds the test scores of similar students in other states. A relatively large share of students in these states come from demographic groups that tend to score less well on NAEP. But, as the results show, these students score better than similar students in other states.

This is illustrated by a simple comparison of fourth-grade reading scores in Oregon and Texas.

Texas limited English proficient (LEP) students perform much better than LEP students in Oregon, and non-LEP students perform about the same in both states. But 21 percent of Texas students are LEP, as compared with 13 percent of Oregon students, so the overall average NAEP score is higher in Oregon than in Texas.13

“NAEP scores will become more useful for making comparisons across states if the underlying data include more detailed information on the characteristics of students and their families. NAEP data currently rely on administrative data from schools and survey responses from students, teachers, and administrators. These data could be augmented by links to other federally held data, such as detailed earnings records on students’ parents maintained by the Internal Revenue Service. Such a linkage would permit significant improvements over the current use of eligibility for the federal free and reduced price lunch program as the main proxy for socioeconomic status.”

Finally, below is the same chart as above that Education Commissioner Morath included in his powerpoint presentation to the School Finance Commission on 1-23-18, but showing data regarding 2013 per pupil spending by various states. https://tea.texas.gov/WorkArea/DownloadAsset.aspx?id=51539619828

As you can see, Texas per-pupil spending is the lowest among the listed states, as well as below the national average.

Other school ranking entities:

Children at risk – includes adjustment for campus demographics in its grading index, counting for 20% of a school’s overall grade: http://childrenatrisk.org/wp-content/uploads/2017/08/C@R-School-Rankings-Webinar-VF1.pdf

Campus Performance Index The Campus Performance Index captures performance on the Student Achievement indicators using values adjusted for the percentage of economically disadvantaged students at each campus. Raw academic measurements, such as those in the Student Achievement Index, have a bias toward campuses with low percentages of economically disadvantaged students. The Campus Performance Index is created to measure the effectiveness of the educators and programs at a campus independent of the differences in the percentage of economically disadvantaged students at each campus. The Campus Performance Index accounts for 20% of elementary, middle, and high school campuses’ overall rank.

http://childrenatrisk-org.vps-texasschoolguide-org.vps.ezhostingserver.com/wp-content/uploads/2017/06/2017-School-Rankings-Methodology-VF.pdf

Other points:

I have not seen the updated Quality Counts 2018 information that you referenced, but relying on the information I found, Texas’s doesn’t appear to have changed much since the 2017 report:

https://www.edweek.org/ew/collections/quality-counts-2018-state-grades/report-card-map-rankings.html?intc=EW-QC18-AP

The Chance-for-Success Index uses a cradle-to-career perspective to examine the role of education in promoting positive outcomes throughout an individual's lifetime. Texas got a C (74.1)(ranking 40th)(39th last year)

The school finance analysis evaluates spending on education and equity in funding across districts within a state. (Tx got a D, 66.8)(ranking 41st)(44th last year)

The K-12 Achievement Index, last updated in 2016, scores states on current academic performance, change over time, and poverty-based gaps. (Tx got a C-, same as state average, and ranks 24th)

As noted by Quality Counts “Nation's Schools Stuck in 'Average' Range on Annual Report Card: The nation's score of 74.5 is about the same as last year, when it posted a 74.2, also a C grade—continuing years of flat performance noted in the annual report, which weighs a host of academic, fiscal, and socioeconomic factors.

For the 11th year in a row, the nation earns a C-plus on the Chance-for-Success Index.”

https://www.edweek.org/ew/articles/2018/01/17/nations-schools-stuck-in-average-range-on.html

More detailed Quality Counts reports coming: According to the 2018 Quality Counts section entitled “a Renewed Focus on the Value of Data”, https://www.edweek.org/ew/articles/2018/01/17/for-quality-counts-a-renewed-focus-on.html “In addition to this issue's grades and articles, the next two Quality Counts reports will offer more specific detail about the factors behind the annual rankings and what the trends mean for the nation's efforts to boost achievement.

In June, for example, we'll examine school spending and finance, including just how evenly that money is spread within states and the role it plays in educational equity.

And in September—as students head back to school—we'll dig into student achievement data, and use Education Week's trademark "Chance for Success" index to show the lifelong impact of factors like family income, parents' education levels, and preschool access.”

From: Selby, Gardner (CMG-Austin)

Sent: Wednesday, January 24, 2018 2:27 PM

Subject: FW: Not adjusting for demographics?

fyi

From: Sterling Lloyd

Sent: Wednesday, January 24, 2018 1:41 PM

To: Selby, Gardner (CMG-Austin)

Subject: RE: Not adjusting for demographics?

Hi Gardner,

While it’s important to consider the impact of demographics when thinking about a state’s academic performance, we try to make our methodology for the grading as straightforward as possible. Adjusting for demographics can make the grading less straightforward for the public. It adds methodological details that need to be evaluated.

Sterling

Sterling C. Lloyd

Assistant Director

Education Week Research Center

(Eaton)

3:11 p.m.

Thanks Gardner. I don’t know if you want a response, but I think this perfectly illustrates the classic tension in performance reporting between the desire for clarity (thus oversimplification) vs accuracy (thus complexity).

Holly