Evolution of Election Polling in the United States
Abstract

Public opinion polls have long played an important role in the study and conduct of elections. In this essay, I outline the evolution of polling as used for three different functions in U.S. presidential elections: forecasting election outcomes, understanding voter behavior, and planning campaign strategy. Since the introduction of scientific polling in the 1936 election, technology has altered the way polls are used by the media, public, candidates, and scholars. Today, polls and surveys remain vital to electoral behavior and our understanding of it, but they are being increasingly supplemented or replaced by alternate measures and methods.

Public opinion polls are now conducted on every topic under the sun—everything from presidential approval to celebrity outfits and sports predictions—but they remain especially fundamental to the conduct and study of elections. Elections and polling are so intertwined that it is hard to imagine one without the other. Poll numbers provide fodder for media coverage and election predictions, they shape candidate and voter behavior, and they are the basis of interpreting the meaning of election outcomes. Public Opinion Quarterly was founded in January 1937 on the heels of the advent of modern scientific polling in U.S. presidential elections. The first issue included an essay, “Straw Polls in 1936,” explaining how George Gallup’s quota-controlled survey of a few thousand triumphed over the Literary Digest’s straw poll of millions in correctly predicting the election outcome (Crossley 1937).

Election polling has evolved considerably since that inaugural issue. Perhaps most notably, there has been an explosion in the number of election polls in the United States. Traugott (2005) estimated a 900-percent increase in trial heat polls between 1984 and 2000. The number has continued to grow since then, due largely to the rise in interactive-voice-response (IVR) and Internet polls since the 2000 election. In the 2008 election, there were an estimated 975 presidential trial heat questions, and well over a million interviews, conducted between Labor Day and Election Day (Panagopolous 2009). It is telling that polling for the next presidential election now begins the day after the previous one. On November 5, 2008, Gallup reported that Sarah Palin led as a potential Republican candidate for the 2012 presidential election.1

There has also been a significant evolution in the nature of election polling. For decades, polls were typically conducted by telephone, using live interviewers, on behalf of media organizations or political candidates. Today, Internet surveys and IVR polls are increasingly common, and polls are often initiated by entrepreneurial pollsters conducting them not for a client, but for self-promotion (Blumenthal 2005). The dissemination of poll numbers has also changed, with many polls now being reported directly on blogs and polling aggregation websites rather than by the traditional media. Journalists are no longer the formal gatekeepers determining if a given poll is of sufficient quality and interest to warrant the public’s attention.

It also seems that we have seen a rise and fall in the credibility of polling since POQ’s inaugural issue. Reflecting on the Literary Digest prediction disaster in the 1936 election, Crossley’s essay asked, “Is it possible to sample public opinion sufficiently accurately to forecast an election, particularly a close one?” (Crossley 1937, p. 24). Crossley argued that it was, provided a representative sample was drawn. Not everyone immediately shared his view, however. It was not until the 1960s and 1970s that surveys became a fixture of political campaigns (J. Converse 1987). Early skepticism that a sample of respondents could say anything about the opinions of millions gave way to a belief in the scientific basis of probability samples. Today, however, nonprobability samples—typically opt-in Internet surveys—are increasingly common, and probability samples are experiencing significant methodological challenges, such as increasing nonresponse and cell-phone-only households. We now hear near constant questioning of the motivation and methods of pollsters, often instigated by partisan bloggers and pundits dissatisfied with the results of a poll. There is, once again, a haze of skepticism surrounding the entire industry.

The role of polling in elections has been the subject of numerous books and articles and has been covered with far more detail, richness, and insight than I can provide here.2 In this essay, I will briefly outline the evolution of polling as used for three different functions in U.S. presidential elections: forecasting election outcomes, understanding voter behavior, and planning campaign strategy.3 The common thread throughout is that technology has altered the way polls are used by the media, public, candidates, and scholars. And while polls and surveys remain vital to electoral behavior and our understanding of it, they are being increasingly supplemented or replaced by alternate measures and methods.

Forecasting Elections

As long as there have been elections, people have tried to predict the outcomes. Before polls, knowledgeable observers, political insiders, and bellwether states were the most commonly used election forecasts (Kernell 2000). Although Gallup’s quota-selected polls in 1936 marked the beginning of scientific election polling, unscientific straw polls date to at least the 1824 presidential election, when informal trial heat tallies were taken in scattered taverns, militia offices, and public meetings (Smith 1990). As the first U.S. presidential election to be decided by popular vote, it was inevitable that people would try to gauge public opinion, however imperfectly. Fast-forward to today, and each new election cycle brings a wave of horserace polling numbers feeding the insatiable appetite of media, bloggers, and political junkies trying to predict the election outcomes.

Unlike most survey research topics, pre-election polls have a truth benchmark—the election results.4 So, after each new election, there is a postmortem assessment of the accuracy of pre-election polls to see how closely the polling industry and individual pollsters matched the official election returns. The reputation of survey firms rests in no small part on these accuracy assessments. The death of the Literary Digest has been attributed to their failed prediction of the 1936 election despite successful predictions from 1916 to 1932 (Squire 1988). More recently, John Zogby, labeled the “prince of pollsters” after nailing the 1996 election prediction, saw his reputation tarnished by poor predictions in subsequent years, with NY Times election blogger Nate Silver more recently calling him “The Worst Pollster in the World” (Silver 2009).

Of course, the very task of assessing accuracy raises questions about how best to measure it. Should a forecast be called “accurate” if it correctly predicts the winner, the winner’s vote share, or the margin of victory? In 1996, CBS News correctly predicted Clinton as the winner, but they forecast an 18-percent margin of victory over Dole, rather than the 8 percent that he actually received. In contrast, Gallup was off by just 2 percentage points in predicting the margin of victory in 2000, but they predicted the wrong winner of the popular vote because they overestimated support for minor-party candidates. In 2004, Fox News nailed Kerry’s vote share of 48 percent, but they incorrectly predicted him to be the winner. As these examples make clear, conclusions about accuracy vary based on the particular yardstick used, and they can be affected by factors like sample size, treatment of undecided and minor-party voters, and field dates (Blumenthal 2008). In the aftermath of the 1948 polling debacle, a group of social scientists—led by Frederick Mosteller—outlined eight different metrics for assessing polling accuracy (Mosteller et al. 1949); more recently, Martin, Traugott, and Kennedy (2005) have added a ninth. Most commonly used are “Mosteller Measure 3,”, the average absolute error on all major candidates between the prediction and the actual results, and “Mosteller Measure 5,” the absolute value of the difference between the margin separating the two leading candidates in the poll and the difference in their margins in the actual vote. Increasingly, scholars are also using Martin et al.’s predictive accuracy measure, which is based on the natural logarithm of the odds ratio of the outcome in the poll and the outcome in the election. By all measures, 2008 was considered a banner year for the polling industry and, by some metrics, could be labeled the most accurate since 1956 (Panagopoulos 2009). As a whole, the polling industry has a strong track record (Traugott 2005), but there have been some embarrassing failures throughout history. Most famously, the polls predicted that Republican Thomas Dewey would beat incumbent Democratic president Harry Truman in the 1948 election. More recently, the polls got it wrong in the 2008 election when they predicted that Barack Obama would defeat Hillary Clinton in the New Hampshire Democratic primary. Predicting election outcomes is a difficult and high-stakes business, so it is important to understand why some polls get it right and some get it wrong.

Like any survey, the quality of predictions can be affected by sampling error and nonsampling errors, including coverage error, nonresponse error, measurement error, processing error, and adjustment error (Groves et al. 2009). It is most recognized that random sampling error can produce fluctuations in polling estimates based on chance alone, simply because a poll includes a sample of respondents rather than the full population. Such error is expressed with the margin of error that is typically reported alongside polling estimates, and the simple (but costly) solution is to increase the sample size. Of greater concern are the systemic errors introduced by the pollsters (or analysts) and respondents that can bias the election forecasts.

Pollsters must make a variety of design decisions—about the mode, timing, sampling method, question formulation, weighting, etc.—and each of these methodological decisions can potentially bias the results. Research has found, for instance, that the number and type (weekend vs. weekday) of days in the field were associated with predictive accuracy, reflecting nonresponse bias (Lau 1994). Mokrzycki, Keeter, and Kennedy (2009) found that telephone polls excluding cell-phone-only households had a slight bias against the Democratic candidates, an illustration of coverage bias. Highlighting the importance of measurement error, Crespi and Morris (1984) demonstrated that question order produced different estimates of candidate support. As other essays in this issue discuss in more detail, there are a wide variety of other methodological decisions that can directly affect data quality; for pre-election polling, the definition of likely voters and the treatment of undecided voters are of particular concern.

Election forecasts can go astray simply because they must predict future behavior. In his 1937 essay, Crossley wrote that “The greatest difficulty of all is the fact that the election itself is not a census, but an application of the sampling principle. Every poll is therefore a sample of a sample” (p. 25). In other words, it is an unknown population to whom pollsters are trying to generalize because we do not know who will show up on Election Day. Different elections can have different cross-sections of voters, a point highlighted in 2008 when an unusually large proportion of minorities and young people turned out to vote for Barack Obama. As such, one of the most consequential methodological decisions made by the pollster or analyst is the selection of likely voters. Forecasts can vary wildly based on the particular method used to define the population of expected voters (Crespi 1988). Erikson, Panagopolous, and Wlezien (2004) found, for instance, that a 19-point swing in support from Gore to Bush in the 2000 presidential campaign was an artifact of Gallup’s likely-voter screen. Every survey firm has its own (often proprietary) method for defining likely voters, typically relying on self-reported measures of voter registration or vote history, but rarely do those models engage the most up-to-date scholarly research on political participation. For instance, pollsters typically use a single likely-voter model for the entire country, but political science research has shown that state-level factors such as registration requirements, early voting rules, and competitiveness can affect an individual’s likelihood of voting.

Once an assumption is made about likely voters, the task of the pre-election poll is to predict how voters will cast their ballots. The standard polling question asks, “If the election were being held today, for whom would you vote?” In making a forecast, pollsters or analysts must make a decision about how to deal with the respondents who say they are undecided, a pool of voters that varies based on the timing of the poll and the methodology used (Fenwick et al. 1982). Hoek and Gendall (1997) found that reducing the proportion of undecided voters through various assignment mechanisms did not necessarily improve the accuracy of estimates. Thus, while it is widely recognized that undecided respondents contribute to polling error, there is still no consensus about what to do with them.

Respondents are another source of error in pre-election polls. An accurate election prediction relies on respondents providing honest answers to the turnout and vote intention questions. Extensive research has documented overreporting of turnout (and turnout intention), primarily the result of social desirability bias (e.g., Belli et al. 1999). In 2008, the presence of an African American on the ticket increased concern that respondents would lie to pollsters about their vote preference. Previous elections had found evidence of a “Bradley effect,” in which pre-election polls overestimate support for a black candidate because white voters tell pollsters they are undecided or will support the black candidate when they do not intend to do so. In the end, research found no evidence that polls systematically overestimated Obama support (Hopkins 2009); in fact, polls were more likely to underestimate support for Obama, likely reflecting higher turnout among groups often not considered likely voters (Silver 2008). Future research should consider the variety of other reasons that respondents might not give incomplete or untruthful answers to the vote choice questions, such as privacy concerns or respondent competence.

Polling predictions can also be jeopardized by individuals changing their minds about their turnout and vote intention between the time of the survey interview and Election Day. Although scholarly research often emphasizes the stability of vote intention, panel data has found that more than 40 percent of respondents change their vote intention at least once during the campaign (Hillygus and Shields 2008). There remains debate, however, about the source of these individual-level dynamics. Gelman and King (1993) argued that movements in poll numbers reflect predictable movement toward the fundamentals, but others have shown that specific campaign events produce movements in the polls (Johnston et al. 1992). In an analysis of the dynamics of pre-election polling, Wlezein and Erikson (2002) attributed as much as 50 percent of the variability in poll numbers simply to sampling error, but they also found that campaign shocks produced real movements—early in the campaign the effects dissipated quickly, but there were smaller, persistent shocks late in the campaign. There remains much to be learned about who in the electorate is most likely to change their minds, when they are most likely to do so, and in response to what stimuli, and such findings will have clear implications for election forecasting. Voter instability is considered the primary explanation for the polling debacle of 1948. Pollsters called the election for Dewey weeks before the election, but a sizeable chunk of voters changed their vote and turnout intention in the final weeks, and they overwhelmingly supported Truman (Crespi 1988). To minimize sources of error, pollsters now continue to do election polling as late as the night before the election, and it is this final poll that is used in the post-election assessments of polling accuracy.5

Unfortunately, it is often difficult to attribute differences in predictions across pollsters to any one factor because of the sheer number of design decisions that are made by each survey firm. One of the greatest obstacles to a better understanding of variation in polling predictions is the lack of methodological transparency. An American Association for Public Opinion Research (AAPOR) committee examining the performance of polls in the 2008 primaries ultimately concluded that they lacked sufficient information to fully assess what went wrong in New Hampshire (Traugott et al. 2009). The experience of the committee helped invigorate a new transparency initiative that urges disclosure of methodological decisions. AAPOR president Peter Miller explained:

Despite decades of work, transparency in public opinion and survey research remains an elusive goal. Often it remains too difficult to get information about how surveys are conducted. Too many researchers do not know how to document their work, or are reluctant to disclose their methods for fear of criticism by non-transparent competitors. Too many significant questions about survey practice remain unaddressed because getting information about how studies are done is onerous or impossible. Too many members of the public have become cynical about survey research because they do not understand how different methods underlie conflicting findings.6

In a world in which there is more variation in polling methodologies, it is more important than ever for survey quality to be evaluated. And this is only possible with methodological transparency. The importance of transparency was recently highlighted by two separate cases in which polling firms were found to have made up or manipulated released survey results during the 2008 election.7

Polling Aggregation

In recognition that individual poll results are subject to random sampling error and any potential biases introduced by a firm’s particular polling methodology, it has become popular to aggregate across many different polls. The widespread availability of poll numbers online has made it easy to do polling aggregation to forecast election outcomes.8 Online poll aggregators include FiveThirtyEight.com, Pollster.com, the Princeton Election Consortium, and RealClearPolitics.com. Nate Silver’s FiveThirtyEight.com, in particular, was a popular sensation in the 2008 campaign; his website reported 3.63 million unique visitors, 20.57 million site visits, and 32.18 million page views in October alone.

Aggregating polls helps reduce volatility in polling predictions. Although there is the tendency for news organizations to focus great attention on every movement up or down in the polls, as Jackman (2005) noted, “media-commissioned polls employ sample sizes that are too small to reliably detect the relatively small day-to-day or week-to-week movements in voter sentiment we would expect to occur over an election campaign” (p. 500). Pooling polls improves the precision of polling estimates—a larger sample size has a smaller margin of error. But there are multiple methods for aggregating polls, and it is not yet clear which one is best.

Some aggregations simply take the average of all available poll numbers. Yet, naïvely pooling across polls ignores house effects, all of the methodological decisions made by a particular survey firm. Unfortunately, this approach might not improve accuracy. Simple averaging assumes that the various sources of bias in the individual poll numbers will cancel each other out, but if biases work in the same direction, aggregation will not reduce bias. For example, if all telephone polls systematically miss cell-phone-only households, shown to be more Democratic-leaning, the aggregation of these polls might produce an estimate that is worse than an individual poll.

Other polling aggregations employ various analytic models that attempt to account for house effects. Some aggregators will exclude certain polls or weight them according to some criteria, for example excluding partisan polls or those deemed to be lower quality. Fivethirtyeight.com weights each poll based on the “pollster’s historical track record, sample size, and recentness of the poll.”9 Still others have used algorithms that incorporate historical trends or other sources of information (e.g., Linzer 2011). Unfortunately, since we cannot assess house effects until after the election, many studies end up making the strong assumption that the average of all polls is a reasonable gauge for house effects (e.g., Jackman 2005). Popular websites have dominated polling aggregation in recent years, but it seems likely that public opinion scholars will increasingly weigh in to help identify the most reasonable methods for combining polls.

STATE VS. NATIONAL POLLING

Also reflecting improvements in polling availability and accessibility, many polling predictions have recently shifted from national-level to state-level analyses. Polling forecasts have historically focused on national-level estimates of the two-party popular vote even though presidential elections are decided by the Electoral College.10 The 2000 election offered a stark reminder of this fact; it became irrelevant that many pollsters had correctly predicted Gore to be the winner of the popular vote because Bush won the Electoral College. With changes in polling technology and costs, the ease of Internet accessibility, and improvements in statistical and computing power, state-level forecasts have since become routine.11 According to NCPP analyses, there were 743 state-level polls in the last two weeks of the 2008 election, compared to 254 during the same time span in 2004 (the first year they evaluated the accuracy of state-level polls).12 Not every state is polled consistently, especially in less competitive states and earlier in the campaign, but forecasts based on state-level polls were updated on an almost daily basis on Pollster.com and 538.com during the 2008 campaign. It seems clear that the future of poll-based election forecasts are aggregations of state-level polls to make Electoral College predictions, but there remains much to be learned about the best methods for aggregating and the best metrics for assessing their accuracy.

On the one hand, state-level forecasts offer a much higher bar for assessing the accuracy of individual pollsters since there are 51 predictions to be made, rather than one. Certainly, we see much greater variability in state-level polls, which do not converge toward the end of the campaign to the same extent as national polls. On the other hand, many pollsters do state-level polling only in a handful of states, and some states are more difficult to predict than others, making it difficult to know the best way to evaluate the accuracy of a given pollster. IVR and Internet polling methods also make up a greater proportion of state level polls—just 53.3 percent were telephone polls with live interviewers in 2008—raising additional questions about the data quality underlying state-based predictions. As state-based forecasts become more common in the coming years, researchers should consider whether the traditional metrics used to evaluate national pre-election polling accuracy remain applicable and informative.

For example, should accuracy assessments be made based on the final, Election Eve polls? It is well established that the accuracy of poll-based predictions improves closer to Election Day, as the number of undecided voters decline and vote preferences stabilize. For example, early polling in the 1992 election showed George H. W. Bush with a healthy lead, but polls converged in predicting Clinton the winner by the end of the campaign. But it is perhaps less interesting to get the forecast correct just before the actual outcome is known. Moreover, the number of early voters—estimated at one-third of the electorate in 2008—further weakens the utility of an Election Eve “prediction.” As an alternative, especially early in the campaign, some have turned to macroeconomic statistical models or prediction markets to make election forecasts.

BEYOND POLLS: MACROECONOMIC MODELS

Beginning in the 1970s, academics developed macroeconomic statistical models using non-polling aggregate data to predict election outcomes (e.g., Fair 1978), and since the 1990s, competing model-based forecasts of the two-party presidential vote have been routinely published by Labor Day before an election (Lewis-Beck 2005). Most statistical models include measures of government and economic performance, though there is debate as to the specific economic indicator to be used—whether GDP growth (Abramowitz 2004), job growth (Lewis-Beck and Tien 2004), inflation rate (Lewis-Beck 2005), or perceptions of personal finances (Holbrook 2004)—and about the inclusion of other variables, like polling numbers and war support, in the statistical model.13 All, however, are based on the basic theory that voters reelect incumbents in good times and they kick the bums out of office in bad times (Fiorina 1981).

Election forecasters argue that statistical predictions should outperform other election predictions because they are rooted in a theory about voter behavior. Lewis-Beck (2005) argued that other forecasting approaches, such as poll- and market-based predictions, “are not based on any theory of the vote. Instead, they are merely providing point estimates on a dependent variable. … It is my belief that, in the long run, the statistical modeling approach, because it draws on voting theory, will yield a better performance” (p. 148).

One criticism of these models is that, like the national-poll-based forecasts, they typically predict the two-party popular vote rather than the Electoral College outcome. Given the two-party system of the United States, the popular vote typically falls within a rather narrow range of values. A naïve prediction based on a coin toss would predict a 50-percent vote share, which gets pretty close to the right answer in many election years. Another criticism is that once we account for the confidence intervals around the point estimate, it becomes evident that most models predict a wide range of possible outcomes, including victory by the opposing candidate (Lewis-Beck 2005). There are only a handful of presidential elections for which the necessary aggregate data are available to estimate the statistical models, so predictions are inherently imprecise. Moreover, according to Greene (1993), the tendency for models to be fitted to previous outcomes—that is, selecting model specification based on past elections—means that the models underestimate the true level of uncertainty. Vavreck (2009) argued that economic models have sometimes failed because they have not taken into account the content of the campaign, especially the candidates’ messages about the economy and their attention to other issues.

Reflecting these issues, the track records for individual models are highly variable; for example, Ray Fair’s model had one of the worst predictions in 1992 despite a previous history of successful forecasts (Greene 1993). Nonetheless, it is clear that there are regularities in presidential races that help in predicting the outcome long before the polling numbers converge on the likely winner. And there is clearly value in being able to make an early prediction, provided that the appropriate amount of uncertainty in that estimate is reported. With the increasing availability of state-level measures and more complex statistical techniques, the field is poised to make further improvements in the accuracy and reliability of model-based forecasts of Electoral College outcomes (e.g., Rigdon et al. 2009).

BEYOND POLLS: PREDICTION MARKETS

Another election-forecasting alternative is prediction markets, such as the Iowa Electronic Market. With such betting markets, people buy and sell candidate futures based on who they think will win the election; for example, on September 1, 2008, Obama futures were selling for $0.60, indicating that traders expected Obama to win 60 percent of the vote in the November election. There was an active and public (although often illegal) election-betting market through much of U.S. history, but it was only recently that prediction markets have been used with any regularity by academic forecasters.14

Relative to polls, the markets rely on very different mechanisms for making a prediction. The key to an accurate poll-based prediction is a representative sample of likely voters and truthful responses to the vote choice question. In contrast, prediction markets aggregate the informed expectations about the winner from those willing to put money behind their judgments. Traders are not representative of likely voters—they tend to be young, male, well educated, and high income—and they do not even need to be eligible to vote (Berg et al. 2001). Traders are just assumed to be making informed judgments.

There is considerable debate in the literature about the benefit of market-based predictions compared to poll-based predictions. Berg et al. (2001) argued that markets are more accurate than polls, but Erikson and Wlezien (2008) showed that the advantage goes away when polls are combined and corrected for a systematic bias. Rothschild (2009) found that when both polls and market predictions were corrected for inherent biases, prediction market forecasts outperformed aggregated polls earlier in the campaign and in more competitive races. Not surprisingly, the accuracy of both improves the closer you get to Election Day.

A related approach is to ask a sample of citizens their expectations about who will win (Lewis-Beck and Tien 1999). Rothschild and Wolfers (2010) argued that it is possible to forecast the winner asking an expectations question (“Who do you think will win?”) of the general public, even without a representative sample survey. Although more research is necessary to understand the conditions under which such an approach might or might not be accurate, it suggests that the addition of an expectations question to pre-election polls could yield a significant advance in the predictive power of polls.

Although prediction markets and statistical models could provide more accurate election forecasts than polls in some circumstances, it is worth noting that polls play an indirect role even here. Many statistical models incorporate presidential approval numbers. And investors no doubt look to poll numbers in making betting decisions.

Even if we found the perfect election forecast—whether from polls, markets, or models—it does little to help us understand the meaning of the election, what issues mattered to the voters, or the dynamics of opinions. Although criticism is most often directed toward tracking polls, election forecasts of all types contribute to the horserace frenzy. As Rosenstiel (2005) argued, media coverage of the horserace comes at the expense of reporting a candidate’s record, policies, or leadership qualities. That is not to say that election forecasting feeds curiosity alone—the anticipated (and actual electoral outcomes) can affect economic decision-making; election results have been shown to be associated with stock prices and exchange rates (Herron et al. 1999). But we generally are interested not only in predicting the election outcomes but also in explaining why a particular candidate has prevailed. And for that task, forecasting models and prediction markets offer little insight; many polls and surveys, however, have real value.

Understanding Voter Behavior

Survey research is the primary tool for answering questions about electoral behavior, including political participation, voter decision-making, public opinion, and campaign effects. Here again, there has been an evolution in our understanding of electoral behavior, and Public Opinion Quarterly has played a fundamental role. Some of the most impactful works in POQ have been not about election prediction, but about explanations of electoral behavior, including the role and influence of party identification (Belknap and Campbell 1951), voter responsiveness to campaign information (Converse 1962), and the impact of the media on voter decision-making (McComb and Shaw 1972) to name a few.

In tracing the development of the voting behavior literature—from the sociological-based analysis of the Columbia School studies to the psychological models of the Michigan School and the rational choice perspective (see Dalton and Wattenberg 1993 for a nice overview)—one of the most striking changes in the field is the extent to which our answers to theoretical questions have been shaped by methodological issues.

For one, it is now widely recognized that our ability to answer substantive questions about an election is directly affected by data-quality issues. As just one example, Burden (2000) found that turnout overreporting in the American National Election Studies has gotten worse in recent decades because of declining response rates among individuals least likely to participate. And turnout overreporting, is itself a classic example of the dangers of measurement error. In recent years, many other self-report measures have come under considerable scrutiny as well. In a comparison of Nielsen estimates and self-reported news exposure, Prior (2009) found that respondents exaggerate news exposure by a factor of 3 on average, with young people overreporting by a factor of 8. Other research has shown that respondents find it socially desirable to say they are politically informed and engaged, in addition to independent, moderate, and tolerant (Holbrook, Green, and Krosnick 2003). Importantly, these data-quality issues matter. Bernstein, Chadha, and Montjoy (2001) found that overreporting has exaggerated the impact of education, partisanship, and religiosity on turnout. Burden (2008) found that the gender gap in party identification shrank when the question wording for party identification emphasized feelings rather than thoughts. Other essays in this issue discuss in more detail the way that sampling, mode, and questionnaire design directly impacts our substantive conclusions. As more and more scholars are collecting original survey data to answer questions about electoral politics, it becomes ever more important for us to have a firm grasp on the way that data-quality issues can affect knowledge claims.

A second methodological evolution in voting behavior research is the recognition that causal relationships are exceptionally difficult to establish using surveys, especially cross-sectional surveys. The most prominent example is the enduring relationship between party identification, issue preferences, and vote choice. There has long been recognition of a strong relationship (Campbell et al. 1960), but the direction of the causal arrow remains unclear. Some research has concluded that party identification is driven by issue positions (Fiorina 1981), while other studies have argued that issue positions stem from party attachments (Jackson 1975) or vote preferences (Page and Brody 1972). There is a chicken-and-egg quality to the literature that has yet to be resolved, but political behavior scholars are no longer naïve to the conundrum. In a recent essay, Bartels (2010) observed the following about the development of voting behavior research:

The search for causal order in voting behavior seemed to have reached an unhappy dead end … few have been content to hope that new theories and statistical wizardry might untangle the causal complexities that emerged in the 1960s and 1970s. Instead, the most common impulse among electoral analysts of the past quarter-century has been to change the subject. Rather than building ever more complex and comprehensive models of individual voting behavior, they have focused on more tractable questions. As a result, contemporary voting research has become increasingly eclectic and opportunistic. (pp. 28–29)

One consequence of these methodological developments is that scholars are increasingly being led away from surveys, especially cross-sectional surveys, in answering substantive questions about electoral behavior. In an effort to make causal claims, some scholars have turned to panel surveys that track the same individual over time. For example, Hillygus and Shields (2008) found that, over the course of a campaign, politically sophisticated individuals changed their vote choice to be in line with their issue positions, even at the expense of party attachments. While such panel analyses can shed light on vote dynamics, they do not typically catch people before their initial policy and party attitudes are formed, so they cannot fully unravel the causal relationships.

Other scholars have turned to experimental designs, where causal relationships can be more cleanly established (e.g., Iyengar and Kinder 1987).15 Field experiments, in particular, have grown in popularity as a tool for gauging the influence of campaign efforts on turnout or vote choice. Gerber and Green (2001), for example, concluded that nonpartisan mobilization appeals by phone did not increase turnout, in contrast to the conclusions of observational research on party contact. Survey experiments, in which experimental designs are embedded in opinion surveys by randomly assigning respondents alternative versions of questionnaire items, are increasingly popular because they combine the generalizability of surveys with the causal leverage of an experiment. Survey experiments have long been used in questionnaire design experiments, but they are now increasingly the basis for testing substantive questions. For example, Krysan (1998) used a survey experiment to show that whites’ racial attitudes vary based on the privacy of their expressed opinions.

Even those using a survey often supplement it with geospatial or administrative data. There is now a large literature on campaign effects that leverages geographic variation in campaign intensity as a proxy for campaign exposure. For example, Huber and Arceneaux (2007) found compelling evidence of the persuasive effects of advertising by linking advertising data to individual-level survey data, and then taking advantage of the fact that some media markets overlap battleground and non-battleground states, exposing some voters to higher levels of advertising than the candidate intended.

Other observational research relies on data about individuals collected not as a result of an interview, but from supplemental data sources, such as administrative records or other electronic databases. We live in an information environment in which data are compiled when we surf the Web, subscribe to a magazine, swipe our credit card, register to vote, and so on—and this information can be used to validate or substitute for survey measures. For example, Meredith (2009) relied on vote history in the voter registration files to examine the habitual nature of voting behavior. Mathiowetz (1992) used employment records to validate occupational self-reports in the Current Population Survey. King (2009) offered the following prediction about the future of voting behavior research:

Instead of trying to extract information from a few thousand activists’ opinions about politics every two years, in the necessarily artificial conversation initiated by a survey interview, we can use new methods to mine the tens of millions of political opinions expressed daily in published blogs. Instead of studying the effects of context and interactions among people by asking respondents to recall their frequency and nature of social contacts, we now have the ability to obtain a continuous record of all phone calls, e-mails, text messages, and in-person contacts among a much larger group. (p. 93)

In sum, public opinion scholars are increasingly cognizant of the important link between substantive questions of interest and the strengths and weaknesses of the methodologies available for answering them. And it is this recognition that has led many scholars to alternative measures of voter attitudes and behaviors.

Candidates and Polling

We see a similar trend in looking at the role of political polling by candidates, parties, and interest groups. Whereas polling was once the primary way to gauge the preferences of the public, today campaigns are increasingly relying on consumer and political databases instead of polling alone (Jacobs and Shapiro 2005).

Before public opinion polling took hold in presidential campaigns (and eventually campaigns for offices at almost all levels), candidates relied on the local party structure to assess the wants and needs of the electorate (Kernell 2000). By the 1960s, public opinion polls were central to campaign strategy, used to determine which issues to emphasize, to test messages, and to identify persuadable voters. Polls became an integral part of a president’s campaign (and term in office), offering an independent read of public opinion and thus autonomy from Congress, the media, and political parties (Eisinger 2003). Jacobs and Shapiro (1995) traced the decisions within the Nixon administration to the institutional development of a White House polling apparatus, and Druckman and Jacobs (2006) showed how those polls shaped strategic policy decisions.

Given changes in the information environment and computing power, candidates today no longer have to generalize from a sample survey to the broader population—they now have information about the entire population. The political parties have built enormous databases that contain information about every registered voter in the United States. Statewide, electronic voter registration files—mandated by the 2002 Help America Vote Act—are the cornerstone of these databases. These files typically include a person’s name, home address, turnout history, party registration, phone number, and other information, and are available to parties and candidates (and, in most states, anyone else who wants it). Consumer, census, political, and polling data are then merged into these files to better predict who is going to turn out, what their beliefs and attitudes are and, ultimately, how they are going to vote.

This development has influenced not only how candidates communicate with citizens, but also whom they are contacting and what they are willing to say. Candidates are able to more efficiently target their resources to particular subsets of the electorate. In doing so, they are particularly likely to ignore those individuals not registered to vote, exacerbating inequalities in political information and political engagement. Hillygus and Shields (2008) showed, for instance, that direct mail, phone calls, and personal visits were directed toward registered voters with an active vote history. Candidates also narrow-cast different messages to different segments of the electorate (Jacobs and Shapiro 2005). In these targeted communications, candidates are taking positions on more issues and more divisive issues than in broadcast messages. In 2004, for instance, the presidential candidates took positions on more than 75 different policy issues in their direct mail, including wedge issues like abortion, gay marriage, and stem cell research that were not mentioned in television advertising (Hillygus and Shields 2008). Polls are still used for assessing a candidate’s standing in the race, but they are just one piece of campaign strategy planning, rather than the foundation.

Conclusion

In the 50th-anniversary issue of Public Opinion Quarterly, Philip Converse observed that “From the very outset in the 1930s, public opinion polling has been closely wedded to the study of popular democratic politics” (1987, p. S12). Election polls are used to predict election outcomes and interpret the meaning of the results. They are the basis for campaign strategy by candidates, parties, and interest groups. They are the primary tool that academics and journalists use to understand voting behavior. At the same time, however, there has been a noticeable decline in the prominence of polls in election politics and scholarship. In forecasting the election, statistical models and prediction markets appear to be viable alternatives to polling predictions, especially early in the campaign. In understanding voting behavior, surveys are increasingly replaced by experimental designs or alternative measures of attitudes and behaviors. In campaign strategy polls are increasingly second fiddle to massive databases from voter files and consumer databases, changing the campaign messages that we see. On the one hand, it seems surprising that the impact of polls might well be declining at the same time their numbers are sharply increasing. On the other hand, it may well be the reason for it. With the proliferation in polls, we have also seen greater variability in the methodologies used and the quality of the data. The lack of transparency about those methodologies has contributed to skepticism about the industry. Coupled with changes in technology and the information environment, it is perhaps no wonder that polls have lost some of their luster.

References

Abramowitz Alan “When Good Forecasts Go Bad: The Time-for-Change Model and the 2004 Presidential Election.” , PS: Political Science and Politics , 2004 , vol. 3 74 (pg. 745 - 46 ) , vol.(pg. Bafumi Joseph Erikson Robert S Wlezien Christopher “Ideological Balancing, Generic Polls, and Midterm Congressional Elections.” , Journal of Politics , 2010 , vol. 72 (pg. 705 - 19 ) , vol.(pg. Bartels Larry “The Study of Electoral Behavior.” , The Oxford Handbook of American Elections and Political Behavior , 2010 Belknap George Campbell Angus “Party Identification and Attitudes Toward Foreign Policy.” , Public Opinion Quarterly , 1951 , vol. 15 (pg. 601 - 23 ) , vol.(pg. Belli Robert F Traugott Michael W Young Margaret McGonagle Katherine A “Reducing Vote Overreporting in Surveys.” , Public Opinion Quarterly , 1999 , vol. 63 (pg. 90 - 108 ) , vol.(pg. Berg Joyce Forsythe Robert Nelson Forrest Rietz Thomas Plott Charles Smith Vernon “Results from a Dozen Years of Election Futures Market Research.” , Handbook of Experimental Economic Results , 2001 Amsterdam Elsevier (pg. 742 - 51 ) (pg. Bernstein Robert Chadha Anita Montjoy Robert “Overreporting Voting: Why It Happens and Why It Matters.” , Public Opinion Quarterly , 2001 , vol. 65 (pg. 22 - 44 ) , vol.(pg. Blumenthal Mark “Toward an Open-Source Methodology: What We Can Learn from the Blogosphere.” , Public Opinion Quarterly , 2005 , vol. 69 5 (pg. 655 - 69 ) , vol.(pg. ——— “NCPP’s Report on Pollster Performance.” , 2008 Burden Barry “Voter Turnout and the National Election Studies.” , Political Analysis , 2000 , vol. 8 (pg. 389 - 98 ) , vol.(pg. ——— “The Social Roots of the Partisan Gender Gap.” , Public Opinion Quarterly , 2008 , vol. 72 (pg. 55 - 75 ) , vol.(pg. Campbell Angus Converse Philip E Miller Warren E Stokes Donald E The American Voter , 1960 New York John Wiley & Sons Converse Jean Survey Research in the United States: Roots and Emergence 1890–1960 , 1987 Berkeley and Los Angeles University of California Press Converse Philip “Information Flow and Stability of Partisan Attitudes.” , Public Opinion Quarterly , 1962 , vol. 26 4 (pg. 578 - 99 ) , vol.(pg. ——— “Changing Conceptions of Public Opinion in the Political Process.” , Public Opinion Quarterly , 1987 Crespi Irving Pre-Election Polling: Sources of Accuracy and Error , 1988 New York Russell Sage Foundation Crespi Irving Morris Dwight “Question Order Effect and the Measurement of Candidate Preference in the 1982 Connecticut Elections.” , Public Opinion Quarterly , 1984 , vol. 48 (pg. 578 - 91 ) , vol.(pg. Crossley Archibald M “Straw Polls in 1936.” , Public Opinion Quarterly , 1937 , vol. 1 January (pg. 24 - 35 ) , vol.(pg. Dalton Russell Wattenberg Martin “The Not So Simple Act of Voting.” , The State of the Discipline , 1993 Durand Claire Blais Andre Larochelle Mylene “The Polls in the 2002 French Presidential Election.” , Public Opinion Quarterly , 2004 , vol. 68 (pg. 602 - 22 ) , vol.(pg. Druckman James Green Donald Kuklinski James Lupia Arthur “The Growth and Development of Experimental Research in Political Science.” , American Political Science Review , 2006 , vol. 100 (pg. 627 - 35 ) , vol.(pg. Druckman James Jacobs Larry R “Lumpers and Splitters: The Public Opinion Information That Politicians Collect and Use.” , Public Opinion Quarterly , 2006 , vol. 70 4 (pg. 453 - 76 ) , vol.(pg. Eisinger Robert M The Evolution of Presidential Polling. , 2003 Cambridge, UK Cambridge University Press Erikson Robert S Panagopoulos Costas Wlezien Christopher “Likely (and Unlikely) Voters and the Assessment of Campaign Dynamics.” , Public Opinion Quarterly , 2004 , vol. 68 4 (pg. 588 - 601 ) , vol.(pg. Erikson Robert S Wlezien Christopher “Are Political Markets Really Superior to Polls as Election Predictions?” , Public Opinion Quarterly , 2008 , vol. 72 (pg. 190 - 215 ) , vol.(pg. Fair Ray C “The Effect of Economic Events on Votes for President.” , Review of Economics and Statistics , 1978 , vol. 60 April (pg. 159 - 73 ) , vol.(pg. Fenwick Ian Wiseman Frederick Becker John Heiman James “Classifying Undecided Voters in Pre-Election Polls.” , Public Opinion Quarterly , 1982 , vol. 46 (pg. 383 - 91 ) , vol.(pg. Fiorina Morris Retrospective Voting in American Elections , 1981 New Haven, CT Yale University Press Gelman Andrew King Gary “Why Are American Presidential Election Campaign Polls So Variable When Votes Are So Predictable?” , British Journal of Political Science , 1993 , vol. 23 October (pg. 409 - 51 ) , vol.(pg. Gerber Alan S Green Donald P “Do Phone Calls Increase Voter Turnout? A Field Experiment.” , Public Opinion Quarterly , 2001 , vol. 65 (pg. 75 - 85 ) , vol.(pg. Greene Jay P “Forewarned Before Forecast: Presidential Election Forecasting Models and the 1992 Election.” , Political Science and Politics , 1993 , vol. 26 (pg. 17 - 21 ) , vol.(pg. Groves Robert M Fowler Floyd J Couper Mick P Lepkowski James M Singer Eleanor Survey Methodology , 2009 2nd ed. Hoboken, NJ John Wiley and Sons Herbst Susan Numbered Voices: How Opinion Polling Has Shaped American Politics , 1993 Chicago University of Chicago Press Herron Michael C Lavin James Cram Donald Silver Jay ‘‘Measurement of Political Effects in the United States Economy: A Study of the 1992 Presidential Election.’’ , Economics and Politics , 1999 , vol. 11 (pg. 51 - 81 ) , vol.(pg. Hillygus D Sunshine Shields Todd The Persuadable Voter , 2008 Princeton, NJ Princeton University Press Hoek Janet Gendall Philip “Factors Affecting Poll Accuracy: An Analysis of Undecided Respondents.” , Marketing Bulletin , 1997 , vol. 8 (pg. 1 - 14 ) , vol.(pg. Holbrook Allyson Green Melanie C Krosnick Jon A Telephone versus Face-to-Face Interviewing of National Probability Samples with Long Questionnaires: Comparisons of Respondent Satisficing and Social Desirability Response Bias , Public Opinion Quarterly , 2003 , vol. 67 (pg. 79 - 125 ) , vol.(pg. Holbrook Thomas DeSart Jay “Using State Polls to Forecast Presidential Election Outcomes in American States.” , International Journal of Forecasting , 1999 , vol. 15 (pg. 137 - 42 ) , vol.(pg. Holbrook Thomas M “Good News for Bush? Economic News, Personal Finances, and the 2004 Presidential Election.” , Political Science and Politics , 2004 , vol. 37 4 (pg. 759 - 61 ) , vol.(pg. Hopkins Daniel J “No More Wilder Effect, Never a Whitman Effect: Why and When Polls Mislead About Black and Female Candidates.” , Journal of Politics , 2009 , vol. 71 3 (pg. 769 - 81 ) , vol.(pg. Huber Greg Arceneaux Kevin “Identifying the Persuasive Effects of Presidential Advertising.” , American Journal of Political Science , 2007 , vol. 51 4 (pg. 957 - 77 ) , vol.(pg. Iyengar Shanto Kinder Donald R News That Matters: Television and American Opinion. , 1987 Chicago University of Chicago Press Jackman Simon “Pooling the Polls over an Election Campaign.” , Australian Journal of Political Science , 2005 , vol. 40 4 (pg. 499 - 517 ) , vol.(pg. Jackson John E “Issues, Party Choices, and Presidential Votes.” , American Journal of Political Science , 1975 , vol. 19 (pg. 161 - 85 ) , vol.(pg. Jacobs Lawrence R Shapiro Robert Y “The Rise of Presidential Polling: The Nixon White House in Historical Perspective.” , Public Opinion Quarterly , 1995 , vol. 59 (pg. 163 - 95 ) , vol.(pg. ——— “Polling Politics, Media, and Election Campaigns.” , Public Opinion Quarterly , 2005 , vol. 69 Special Issue No. 5 (pg. 635 - 41 ) , vol.(pg. Johnston Richard Blais Andre Brady Henry E Crete Jean Letting the People Decide: Dynamics of a Canadian Election , 1992 Stanford, CA Stanford University Press Kernell Samuel “Life Before Polls: Ohio Politicians Predict the Presidential Vote.” , Political Science and Politics , 2000 , vol. 33 (pg. 569 - 74 ) , vol.(pg. King Gary King Gary Schlozman Kay Norman Nie “The Changing Evidence Base of Social Science Research.” , The Future of Political Science: 100 Perspectives , 2009 New York Routledge Press Krysan Maria “Privacy and the Expression of White Racial Attitudes: A Comparison Across Three Contexts.” , Public Opinion Quarterly , 1998 , vol. 62 (pg. 506 - 44 ) , vol.(pg. Lau Richard R “An Analysis of the Accuracy of ‘Trial Heat’ Polls in the 1992 Presidential Election.” , Public Opinion Quarterly , 1994 , vol. 58 Spring (pg. 2 - 20 ) , vol.(pg. Lewis-Beck Michael “Election Forecasting: Principles and Practice.” , British Journal of Political Science 7(2):145–64 , 2005 Lewis-Beck Michael Tien Charles “Voters as Forecasters: A Micromodel of Election Prediction.” , International Journal of Forecasting , 1999 , vol. 15 (pg. 175 - 84 ) , vol.(pg. ——— “Jobs and the Job of President: A Forecast for 2004.” , Political Science and Politics , 2004 , vol. 37 4 (pg. 753 - 58 ) , vol.(pg. Linzer Drew “Dynamic Bayesian Forecasting of Presidential Elections in the States.” , Working Paper , 2011 Emory University Martin Elizabeth A Traugott Michael W Kennedy Courtney “A Review and Proposal for a New Measure of Poll Accuracy.” , Public Opinion Quarterly , 2005 , vol. 69 Autumn (pg. 342 - 69 ) , vol.(pg. Mathiowetz Nancy “Errors in Reports of Occupations.” , Public Opinion Quarterly , 1992 , vol. 56 (pg. 352 - 55 ) , vol.(pg. Meredith Marc “Persistence in Political Participation.” , Quarterly Journal of Political Science , 2009 , vol. 4 3 (pg. 186 - 208 ) , vol.(pg. McComb Maxwell Shaw Donald “Agenda-Setting Function of Mass Media.” , Public Opinion Quarterly , 1972 , vol. 36 (pg. 176 - 87 ) , vol.(pg. Mokrzycki Michael Keeter Scott Kennedy Courtney “Cell-Phone-Only Voters in the 2008 Exit Polls and Implications for Future Noncoverage Bias.” , Public Opinion Quarterly , 2009 , vol. 73 5 (pg. 845 - 65 ) , vol.(pg. Mosteller Frederick Hyman H McCarthy P Marks E Truman D The Pre-Election Polls of 1948: Report to the Committee on Analysis of Pre-Election Polls and Forecasts , 1949 New York Social Science Research Council Page Benjamin I Brody Richard A “Policy Voting and the Electoral Process: The Vietnam Issue.” , American Political Science Review , 1972 , vol. 66 (pg. 979 - 95 ) , vol.(pg. Panagopolous Costas “Polls and Elections: Pre-Election Accuracy in the 2008 General Elections.” , Presidential Studies Quarterly , 2009 , vol. 39 December (pg. 896 - 907 ) , vol.(pg. Prior Markus “The Immensely Inflated News Audience: Assessing Bias in Self-Reported News Exposure.” , Public Opinion Quarterly , 2009 , vol. 73 1 (pg. 130 - 43 ) , vol.(pg. Rhode Paul W Strumpf Koleman S “Historic Presidential Betting Markets.” , Journal of Economic Perspectives , 2004 , vol. 18 Spring (pg. 127 - 42 ) , vol.(pg. Rigdon SE Jacobson SH Cho WKT Sewell EC Rigdon CJ “A Bayesian Prediction Model for the U.S. Presidential Election.” , American Politics Research , 2009 , vol. 37 (pg. 700 - 24 ) , vol.(pg. Rosenstiel Tom “Political Polling and the New Media Culture: A Case of More Being Less.” , Public Opinion Quarterly , 2005 , vol. 69 5 (pg. 698 - 715 ) , vol.(pg. Rothschild David “Forecasting Elections: Comparing Prediction Markets, Polls, and Their Biases.” , Public Opinion Quarterly , 2009 , vol. 73 5 (pg. 895 - 916 ) , vol.(pg. Rothschild David Wolfers Justin “Forecasting Elections: Voter Intentions versus Expectations.” , 2010 Silver Nate “Debunking the Bradley Effect.” , Newsweek , 2008 ——— “The Worst Pollster in the World Strikes Again.” , 2009 Smith Tom W “The First Straw? A Study of the Origins of Election Polls.” , Public Opinion Quarterly , 1990 , vol. 54 1 (pg. 21 - 36 ) , vol.(pg. Squire Peverill “Why the Literary Digest Failed.” , Public Opinion Quarterly , 1988 , vol. 52 (pg. 125 - 33 ) , vol.(pg. Traugott Michael “The Accuracy of the National Pre-Election Polls in the 2004 Presidential Election.” , Public Opinion Quarterly , 2005 , vol. 65 5 (pg. 642 - 54 ) , vol.(pg. Traugott Michael Bolger Glenn Davis Darren W Franklin Charles Groves Robert M Lavrakas Paul J Mellman Mark S Meyer Philip Olson Kristen Selzer JAnn Wlezien Chris “An Evaluation of the Methodology of the 2008 Pre-Election Primary Polls.” , 2009 Vavreck Lynn The Message Matters: The Economy and Presidential Campaigns. , 2009 Princeton, NJ Princeton University Press Wand Jonathan Shotts Kenneth Sekhon Jasjeet S Mebane Walter R Jr Jr Herron Michael Brady Henry E “The Butterfly Did It: The Aberrant Vote for Buchanan in Palm Beach County, Florida.” , American Political Science Review , 2001 , vol. 95 (pg. 793 - 810 ) , vol.(pg. Wlezien Christopher Erikson Robert S “The Timeline of Presidential Election Campaigns.” , Journal of Politics , 2002 , vol. 64 4 (pg. 969 - 93 ) , vol.(pg.

© The Author 2011. Published by Oxford University Press on behalf of the American Association for Public Opinion Research. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com